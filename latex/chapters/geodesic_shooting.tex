\section{Algorithm: Geodesic Shooting}

The goal of this section is to reproduce the experiments in \cite{owhadi20} (is this really the goal?).
Coarse structure:
\begin{enumerate}
	\item Introduction/transition to the algorithm - motivated by the infinite depth limit
	\item A proper motivation is needed though
	\item The algorithm: Step by step
	\item Notes on implementation: Automatic gradients, tikhonov regularization, optimizer, convergence
	\item Experimental results: Dataset, kernel choice, Images with different parameters, maybe something like precision?
\end{enumerate}

Instead of $\mathbb{R}^{n\times2}$ $\mathbb{R}^{2n}$.
We utilize the Block Operator Matrix Notation, which makes calculations particularly easy as everything can be written over $\mathbb{R}^{n\times n}$ (more or less).
Proof that we can then use standard matrix multiplication.




\subsection{An Algorithm based on Mechanical Regression}

In \cref{theo:geodesic-shooting} (and visually in \cref{fig:convergence}) we saw an equivalent formulation of the optimization problem modeled by ResNets.
This gives rise to a new idea for an algorithm which solves the mechanical regression task by geodesic shooting.
Given the data points $X$ and $Y$, the basic idea is as follows:
\begin{enumerate}[label=\arabic*.]
	\item Choose a loss function $l$, kernels $\Gamma$ and $K$ and a balancing parameter $\nu$.
	\item Choose an initial momentum $p_0$.
	\item Minimize $\frac{\nu}{2} p(0)^\T \mathbf{\Gamma}(X, X)p(0) + l(q(1), Y)$ by iterating the following steps:
	\begin{enumerate}[label=\arabic*.]
		\item Solve the Hamiltonian system \ref{eq:hamiltonian-system} with initial conditions $q(0) = X$, $p(0) = p_0$.
		\item Compute $q(1)$ and with that, the gradient of $\frac{\nu}{2} p(0)^\T \mathbf{\Gamma}(X, X)p(0) + l(q(1), Y)$ with respect to $p(0)$.
		\item Update $p(0)$ accordingly.
	\end{enumerate}
	\item Solve the differential equation \ref{eq:phi-v-differential-equation} with $q(0) = X$ and the optimal momentum $p(0)$ computed above to obtain $\Phi^v(\cdot, 1): \cX \rightarrow \cX$.
	\item Compute $f: \cX \rightarrow \cY$ as the minimizer of $l(\Phi^v(X, 1), Y)$ and approximate the target function $f^\dagger$ with $f \circ \Phi^v(\cdot, 1)$.
\end{enumerate}

We will now examine the individual steps in depth.
The first two steps are fairly straight forward.
Viable choices for the loss are, of course, the optimal recovery and the ridge regression loss, which includes common losses such as the empirical squared error.
The kernels can be chosen arbitrarily, in theory every function satisfying the definition is possible.
In classical machine learning, popular kernel choices (e.g. for Support Vector Machines (SVM)) include linear, polynomial, Gaussian/Radial Basis Function (RBF), sigmoid and ANOVA kernels.

The first difficulties arise when implementing step 3.1, which involves solving the Hamiltonian system \ref{eq:hamiltonian-system}.
This system cannot be solved analytically and thus has to be approximated.
For solving (systems of) differential equations, many numerical integration schemes exist, some of them preserving different invariants.
The Hamiltonian system we are dealing with has many invariants:
We have already seen that the energy is preserved, other invariants are the canonical symplectic 2-form \cite{marsden10} and the area/volume in the so called phase space (the space in which $(q(t), p(t))$ lies) \cite{hairer06}.
In their implementation, \citet{owhadi20} use a slightly modified version of the classical Störmer-Verlet method (also known as leapfrog method)\footnote{This method even was used by Newton in his \emph{Principia Mathematica} to prove Kepler's second law \cite{hairer03}.}.
However, the Hamiltonian, here, is non-separable.
This means that it cannot be written as $\cH(q, p) = T(p) + U(q)$, which is often possible in physical applications, where $T$ corresponds to the kinetic and $U$ to the potential energy of the system.
Non-separability of the Hamiltonian implies that the Störmer-Verlet scheme is implicit \cite{hairer06}, which makes it difficult to implement and computationally inefficient.
Thus \citet{owhadi20} use a modified scheme, which is given by
\begin{align}
	p_{n+\frac{1}{2}} &\gets p_n - \frac{h}{2} \grad_q\fH\left(q_n, p_n\right) \label{eq:leapfrog-p.5}\\
	q_{n+1} &\gets q_n + h \grad_p\fH\left(q_n, p_{n+\frac{1}{2}}\right)\\
	p_{n+1} &\gets p_{n+\frac{1}{2}} - \frac{h}{2} \grad_q\fH\left(q_{n+1}, p_{n+\frac{1}{2}}\right) \label{eq:leapfrog-p1}\ .
\end{align}
Note the difference in the second line, where usually 
\begin{equation}
q_{n+1} \gets q + \frac{h}{2} \left(\grad_p\fH\left(q_n, p_{n+\frac{1}{2}}\right) + \grad_p\fH\left(q_{n+1}, p_{n+\frac{1}{2}}\right)\right) \ .
\end{equation}
Whereas the modified scheme lost its symplecticity, it is explicit, time-reversible and shows sufficient stability.

Here comes a little something on step 4.

In the last step, the computability of $f$ strongly depends on the chosen loss.
For losses like optimal recovery or ridge regression, we already saw in \cref{eq:ridge-regression-f,eq:optimal-recovery-f} that $f$ can be explicitly determined.
In other cases there might not exist closed form solutions.\Todo{Can we have an example for that?}

\input{figures/figure_algorithm}

\cref{fig:algo} shows a pseudo code implementation of step 3.
As we are trying to approximate $q(1)$, the step size $h$ of the modified Leapfrog integration scheme explained above should ideally be chosen such that $\frac{1}{h} \in \mathbb{N}$.
The call \texttt{optimizer.minimize()} performs one step of the utilized minimization algorithm.
Among the viable choices for such an algorithm are variants of gradient descent, such as Stochastic Gradient Descent or Adam.
Concerning the while-loop's condition: Of course, \Todo{Maybe fix this?}Line \ref{code:p0-convergence} cannot be implemented effectively as such.
One alternative option would be to constantly monitor the target function's value $v$ and stop optimizing when it plateaus, in the hope of having found a minimum.

\subsection{Experiments}

In this section some experimental results of \citet{owhadi20}'s algorithm are described.

\subsubsection{Dataset and Hyperparameters}

For the experiments the two-dimensional \emph{Swiss Roll} dataset is used.
This dataset is a common benchmark in machine learning.
It consists of two interleaved spirals, one belonging to class $A$, the other to class $B$.
The spirals were sampled equidistantly with $100$ points each spanning a total of approximately two coils.
Class $A$ was assigned the value $1$ and $B$ the value $-1$.
Additionally, some Gaussian noise was added to the sampled points in order to mimic imprecisions found in real world datasets.
The result of this process are pairs $(X_i, Y_i)$, $i \in [200]$, with $X_i \in \R^2$ and $Y_i \in \R$; so continuing the nomenclature of the previous sections, we have $\cX = \R^2$ and $\cY = \R$.
A depiction of the dataset can be seen in \cref{fig:dataset}. 
\input{figures/figure_dataset}

Recall the RKHSs $\cV \subseteq \{\cX \rightarrow \cX\}$ and $\cH \subseteq \{\cX \rightarrow \cY\}$ with the respective kernels $\Gamma$ and $K$.
In order to implement the algorithm, we have to choose those kernels.
For both $K$ and $\Gamma$ variants of a Gaussian kernel (also known as Gaussian Radial Basis Function kernel) were used.
These kinds of kernels have proven to be well-suited for a multitude of applications in machine learning and are especially popular choices for SVMs.\Todo{Ref for applications!} 
Formally, for $x_1, x_2 \in \cX$ the kernels are defined as
\begin{align}
	\Gamma(x_1, x_2) &\coloneqq \left(e^{-\frac{\norm{x_1 - x_2}_\cX^2}{s^2}} + r\right) \cdot \mathbbm{1}_2\\
	K(x_1, x_2) &\coloneqq e^{-\frac{\norm{x_1 - x_2}_\cX^2}{s^2}} + r \ .
\end{align}
$\mathbbm{1}_2$ is the $(2\times2)$-identity matrix and $s, r \in \R$ the kernels' parameters which we set to $s = 5$ and $r = 0.1$ for both.
Note that both kernels are well-defined in the sense that they are (isomorphic to) functions $\Gamma: \cX \times \cX \rightarrow L(\cX, \cX)$ and $K: \cX \times \cX \rightarrow L(\cY, \cY)$ and satisfy all properties required of them in \cref{def:kernel} (which we will not proof here).


\subsubsection{Results}

Here will be a figure comparing the flows with different parameters.

\subsubsection{Remarks on the Implementation}

The pseudocode in \cref{fig:algo} can be implemented fairly straight forward.
However, there are some tricks that make it easier, some of which are presented here.

The first simplification lies in the implementation of the inner product on $\cX = \R^2$, which is just the standard scalar product.
In order to fully make use of modern tensor-processing frameworks' parallel processing abilities, expressing the scalar product of $\cX^N$ (which is defined as $\sum_{i=1}^N \left<x_{1, i}, x_{2, i}\right>_\cX$ for $x_1, x_2 \in \cX^N$) in terms of matrix operations is highly desirable.
\cref{cor:matrix-ring-equivalence} provides an option to do so.
We simply (bijectivly) map vectors from $\cX^N$ to $\R^{2N}$.
Then we can perform standard vector-vector multiplication efficiently using the framework's capabilities and, if required, map the result back to $\cX^N$.
The application of a block operator matrix to a vector can be performed similarly, employing \cref{cor:matrix-vector-equivalence}:
Block operator matrices are mapped from $L(\cX^N, \cX^N)$ to $\R^{2N \times 2N}$ and subsequently standard matrix-vector multiplication can be carried out.
It turned out that it is easier to work in the spaces $\R^{2N}$ and $\R^{2N \times 2N}$ by default and switch to the isomorphic ones only when necessary.

Another potentially difficult step to implement is the gradient of $\fH$ with respect to $q$, especially in cases like \cref{eq:leapfrog-p.5,eq:leapfrog-p1} where no closed form formula is available.
A solution for this would be to approximate the gradient using two-point methods such as the left/right sided or symmetric difference quotient.
However, some modern programming languages or frameworks therein (like the Julia language or Tensorflow and Pytorch for Python) provide automatic forward and backward differentiating capabilities.
Those are not only simpler to implement but also computationally more efficient, which strongly encourages their use.

As the experiments showed, using the optimal recovery loss in its pure form does not seem to be possible (at least for the Swiss Roll dataset).
Recall \cref{eq:optimal-recovery-loss-closed-form} where we identified the loss as $l(X, Y) = Y^\T \mathbf{K}(X, X)^{-1} Y$.
Note that it involves computing $\mathbf{K}(X, X)^{-1}$, or rather solving the linear system $\mathbf{K}(X, X)^{-1} Y$.
That system, involving a $(200\times200)$ matrix, proved to be ill-conditioned, which gave rise to some numerical issues and compromised the minimization process.
Hence, some matrix regularization was applied:
Adding a small perturbation $\lambda$ to the diagonal elements (in the experiments $\lambda = 10^{-6}$ was used) alleviated this problem.
This method is also known as Tikhonov regularization.
However, this effectively changed the loss to the ridge regression loss employing the empirical squared error, which reads (\cref{eq:ridge-regression-loss-se-closed-form}):
\begin{equation}
	l(X, Y) = \lambda Y^\T (\mathbf{K}(X, X) + \lambda \mathbbm{1}_N)^{-1} Y \ ,
\end{equation}
the only difference being the initial scaling by $\lambda$.

The Python implementation used in the experiments is available at \url{https://github.com/NKlug/thesis-mechanical-regression}.
The use of Google's Tensorflow framework \cite{tensorflow15} made it particularly easy to leverage GPU acceleration, automated gradients and pre-implemented optimization algorithms.
