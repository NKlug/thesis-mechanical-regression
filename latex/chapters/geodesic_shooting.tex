\section{Algorithm: Geodesic Shooting}

The goal of this section is to reproduce the experiments in \cite{owhadi20} (is this really the goal?).
Coarse structure:
\begin{enumerate}
	\item Introduction/transition to the algorithm - motivated by the infinite depth limit
	\item A proper motivation is needed though
	\item The algorithm: Step by step
	\item Notes on implementation: Automatic gradients, tikhonov regularization, optimizer, convergence
	\item Experimental results: Dataset, kernel choice, Images with different parameters, maybe something like precision?
\end{enumerate}

Instead of $\mathbb{R}^{n\times2}$ $\mathbb{R}^{2n}$.
We utilize the Block Operator Matrix Notation, which makes calculations particularly easy as everything can be written over $\mathbb{R}^{n\times n}$ (more or less).
Proof that we can then use standard matrix multiplication.

\input{figures/figure_algorithm}



Dataset: We use a common benchmark dataset, the Swiss Rolls Dataset.
It can be generated as two interleaved spirals, one spiral belonging to class A, the other to class B.

\subsection{An Algorithm based on Mechanical Regression}

In \cref{theo:geodesic-shooting} (and visually in \cref{fig:convergence}) we saw an equivalent formulation of the optimization problem modeled by ResNets.
This gives rise to a new idea for an algorithm which solves the mechanical regression task by geodesic shooting.
Given the data points $X$ and $Y$, the basic idea is as follows:
\begin{enumerate}[label=\arabic*.]
	\item Choose a loss function $l$, kernels $\Gamma$ and $K$ and a balancing parameter $\nu$.
	\item Choose an initial momentum $p(0)$.
	\item Minimize $\frac{\nu}{2} p(0)^\T \mathbf{\Gamma}(X, X)p(0) + l(q(1), Y)$ by iterating the following steps:
	\begin{enumerate}[label=\arabic*.]
		\item Solve the Hamiltonian system \ref{eq:hamiltonian-system} with initial conditions $q(0) = X$, $p(0) = p0$.
		\item Compute $q(1)$ and with that, the gradient of $\frac{\nu}{2} p(0)^\T \mathbf{\Gamma}(X, X)p(0) + l(q(1), Y)$ with respect to $p(0)$.
		\item Update $p(0)$ accordingly.
	\end{enumerate}
	\item Solve the differential equation \ref{eq:phi-v-differential-equation} with $q(0) = X$ and $p(0)$ the optimal momentum computed above to obtain $\Phi^v(\cdot, 1): \cX \rightarrow \cX$.
	\item Compute $f: \cX \rightarrow \cY$ as the minimizer of $l(\Phi^v(X, 1), Y)$ and approximate the target function $f^\dagger$ with $f \circ \Phi^v(\cdot, 1)$.
\end{enumerate}

We will now examine the individual steps in depth.
The first two steps are fairly straight forward.
Viable choices for the loss are, of course, the optimal recovery and the ridge regression loss, which includes common losses such as the empirical squared error.
The kernels can be chosen arbitrarily, in theory every function satisfying the definition is possible.
In classical machine learning, popular kernel choices (e.g. for Support Vector Machines) include linear, polynomial, Gaussian/Radial Basis Function (RBF), sigmoid and ANOVA kernels.

The first difficulties arise when implementing step 3.1, which involves solving the Hamiltonian system \ref{eq:hamiltonian-system}.
This system cannot be solved exactly and thus has to be approximated.
For solving (systems of) differential equations, many integration schemes exist, some of them preserving different invariants.
The Hamiltonian system we are dealing with has many invariants:
We have already seen that the energy is preserved, other invariants are the canonical symplectic 2-form \cite{marsden10} and the area/volume in the so called phase space (the space in which $(q(t), p(t))$ lies) \cite{hairer06}.
In their implementation \citet{owhadi20} use a slightly modified version of the classical Störmer-Verlet method (also known as leapfrog method)\footnote{This method was even used by Newton in his \emph{Principia Mathematica} \cite{hairer03}.}.
However, as the Hamiltonian, here, is non-separable.
That means that it cannot be written as $\cH(q, p) = T(p) + U(q)$, which is often possible in physical applications, where $T$ corresponds to the kinetic and $U$ to the potential energy of the system.
Non-separability of the Hamiltonian implies that the Störmer-Verlet scheme is implicit \cite{hairer06}, which makes it difficult to implement and computationally inefficient.
Thus \citet{owhadi20} use a modified scheme, which is given by
\begin{align}
	p_{n+\frac{1}{2}} &\gets p_n - \frac{h}{2} \frac{\partial}{\partial q}\fH\left(q_n, p_n\right) \\
	q_{n+1} &\gets q + h \frac{\partial}{\partial q}\fH\left(q_n, p_{n+\frac{1}{2}}\right)\\
	p_{n+1} &\gets p - \frac{h}{2} \frac{\partial}{\partial q}\fH\left(q_{n+1}, p_{n+\frac{1}{2}}\right) \ .
\end{align}
Note the difference in the second line, where usually 
\begin{equation}
q_{n+1} \gets q + \frac{h}{2} \left(\frac{\partial}{\partial q}\fH\left(q_n, p_{n+\frac{1}{2}}\right) + \frac{\partial}{\partial q}\fH\left(q_{n+1}, p_{n+\frac{1}{2}}\right)\right) \ .
\end{equation}
Whereas the modified scheme lost its symplecticity, it is explicit, time-reversible and shows sufficient stability.

Here comes a little something on step 4.

In the last step, the computability of $f$ strongly depends on the chosen loss.
For losses like optimal recovery or ridge regression, we already saw in \cref{eq:ridge-regression-f,eq:optimal-recovery-f} how $f$ can be explicitly determined.



This subsection contains
\begin{itemize}
	\item Pseudo code algorithm
	\item explanation of the algorithm and it's steps
	\item Note on the leapfrog integration scheme and why its used
\end{itemize}

\subsection{Notes on the Implementation}

For $\bGamma$ and $K$ we choose variants of the general purpose Gaussian kernel:...

The implementation of the algorithm used in the experiments is available at \href{https://github.com/NKlug/thesis-mechanical-regression}{https://github.com/NKlug/thesis-mechanical-regression}.

Tikhonov regularization was necessary!

\subsection{Experimental Results}

Here will be a figure comparing the flows with different parameters.

