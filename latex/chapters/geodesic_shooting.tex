\section{Algorithm: Geodesic Shooting}

The goal of this section is to reproduce the experiments in \cite{owhadi20} (is this really the goal?).
Coarse structure:
\begin{enumerate}
	\item Introduction/transition to the algorithm - motivated by the infinite depth limit
	\item A proper motivation is needed though
	\item The algorithm: Step by step
	\item Notes on implementation: Automatic gradients, tikhonov regularization, optimizer, convergence
	\item Experimental results: Dataset, kernel choice, Images with different parameters, maybe something like precision?
\end{enumerate}

Instead of $\mathbb{R}^{n\times2}$ $\mathbb{R}^{2n}$.
We utilize the Block Operator Matrix Notation, which makes calculations particularly easy as everything can be written over $\mathbb{R}^{n\times n}$ (more or less).
Proof that we can then use standard matrix multiplication.




\subsection{An Algorithm based on Mechanical Regression}

In \cref{theo:geodesic-shooting} (and visually in \cref{fig:convergence}) we saw an equivalent formulation of the optimization problem modeled by ResNets.
This gives rise to a new idea for an algorithm which solves the mechanical regression task by geodesic shooting.
Given the data points $X$ and $Y$, the basic idea is as follows:
\begin{enumerate}[label=\arabic*.]
	\item Choose a loss function $l$, kernels $\Gamma$ and $K$ and a balancing parameter $\nu$.
	\item Choose an initial momentum $p(0)$.
	\item Minimize $\frac{\nu}{2} p(0)^\T \mathbf{\Gamma}(X, X)p(0) + l(q(1), Y)$ by iterating the following steps:
	\begin{enumerate}[label=\arabic*.]
		\item Solve the Hamiltonian system \ref{eq:hamiltonian-system} with initial conditions $q(0) = X$, $p(0) = p0$.
		\item Compute $q(1)$ and with that, the gradient of $\frac{\nu}{2} p(0)^\T \mathbf{\Gamma}(X, X)p(0) + l(q(1), Y)$ with respect to $p(0)$.
		\item Update $p(0)$ accordingly.
	\end{enumerate}
	\item Solve the differential equation \ref{eq:phi-v-differential-equation} with $q(0) = X$ and the optimal momentum $p(0)$ computed above to obtain $\Phi^v(\cdot, 1): \cX \rightarrow \cX$.
	\item Compute $f: \cX \rightarrow \cY$ as the minimizer of $l(\Phi^v(X, 1), Y)$ and approximate the target function $f^\dagger$ with $f \circ \Phi^v(\cdot, 1)$.
\end{enumerate}

We will now examine the individual steps in depth.
The first two steps are fairly straight forward.
Viable choices for the loss are, of course, the optimal recovery and the ridge regression loss, which includes common losses such as the empirical squared error.
The kernels can be chosen arbitrarily, in theory every function satisfying the definition is possible.
In classical machine learning, popular kernel choices (e.g. for Support Vector Machines (SVM)) include linear, polynomial, Gaussian/Radial Basis Function (RBF), sigmoid and ANOVA kernels.

The first difficulties arise when implementing step 3.1, which involves solving the Hamiltonian system \ref{eq:hamiltonian-system}.
This system cannot be solved exactly and thus has to be approximated.
For solving (systems of) differential equations, many integration schemes exist, some of them preserving different invariants.
The Hamiltonian system we are dealing with has many invariants:
We have already seen that the energy is preserved, other invariants are the canonical symplectic 2-form \cite{marsden10} and the area/volume in the so called phase space (the space in which $(q(t), p(t))$ lies) \cite{hairer06}.
In their implementation, \citet{owhadi20} use a slightly modified version of the classical Störmer-Verlet method (also known as leapfrog method)\footnote{This method even was used by Newton in his \emph{Principia Mathematica} to prove Kepler's second law \cite{hairer03}.}.
However, the Hamiltonian, here, is non-separable.
This means that it cannot be written as $\cH(q, p) = T(p) + U(q)$, which is often possible in physical applications, where $T$ corresponds to the kinetic and $U$ to the potential energy of the system.
Non-separability of the Hamiltonian implies that the Störmer-Verlet scheme is implicit \cite{hairer06}, which makes it difficult to implement and computationally inefficient.
Thus \citet{owhadi20} use a modified scheme, which is given by
\begin{align}
	p_{n+\frac{1}{2}} &\gets p_n - \frac{h}{2} \frac{\partial}{\partial q}\fH\left(q_n, p_n\right) \\
	q_{n+1} &\gets q_n + h \frac{\partial}{\partial q}\fH\left(q_n, p_{n+\frac{1}{2}}\right)\\
	p_{n+1} &\gets p_{n+\frac{1}{2}} - \frac{h}{2} \frac{\partial}{\partial q}\fH\left(q_{n+1}, p_{n+\frac{1}{2}}\right) \ .
\end{align}
Note the difference in the second line, where usually 
\begin{equation}
q_{n+1} \gets q + \frac{h}{2} \left(\frac{\partial}{\partial q}\fH\left(q_n, p_{n+\frac{1}{2}}\right) + \frac{\partial}{\partial q}\fH\left(q_{n+1}, p_{n+\frac{1}{2}}\right)\right) \ .
\end{equation}
Whereas the modified scheme lost its symplecticity, it is explicit, time-reversible and shows sufficient stability.

Here comes a little something on step 4.

In the last step, the computability of $f$ strongly depends on the chosen loss.
For losses like optimal recovery or ridge regression, we already saw in \cref{eq:ridge-regression-f,eq:optimal-recovery-f} that $f$ can be explicitly determined.
In other cases there might not exist closed form solutions.\Todo{Can we have an example for that?}

\input{figures/figure_algorithm}

\cref{fig:algo} shows a pseudo code implementation of step 3.
As we are trying to approximate $q(1)$, the step size $h$ of the modified Leapfrog integration scheme explained above should ideally be chosen such that $\frac{1}{h} \in \mathbb{N}$.
The call \texttt{optimizer.minimize()} performs one step of the utilized minimization algorithm.
Among the viable choices for such an algorithm are variants of gradient descent, such as Stochastic Gradient Descent or Adam.

\subsection{Experiments}

In this section some experimental results of \citet{owhadi20}'s algorithm are described.

\subsubsection{Dataset and Hyperparameters}

For the experiments the two-dimensional \emph{Swiss Roll} dataset is used.
This dataset is a common benchmark in machine learning.
It consists of two interleaved spirals, one belonging to class $A$, the other to class $B$.
The spirals were sampled equidistantly with 100 points each spanning a total of approximately two coils.
Class $A$ was assigned the value $1$ and $B$ the value $-1$.
Additionally, some Gaussian noise was added to the sampled points in order to mimic imprecisions found in real world datasets.
The result of this process are pairs $(X_i, Y_i)$, $i \in [200]$, with $X_i \in \R^2$ and $Y_i \in \R$; so using the nomenclature of the previous sections, we have $\cX = \R^2$ and $\cY = \R$.
A depiction of the dataset can be seen in \cref{fig:dataset}. 
\input{figures/figure_dataset}

Recall the RKHSs $\cV \subseteq \{\cX \rightarrow \cX\}$ and $\cH \subseteq \{\cX \rightarrow \cY\}$ with the respective kernels $\Gamma$ and $K$.
In order to implement the algorithm, we have to choose those kernels.
For both $K$ and $\Gamma$ we choose variants of a a Gaussian kernel (also known as Gaussian Radial Basis Function kernel).
These kinds of kernels have proven to be well-suited for a multitude of applications in machine learning and are especially popular choices for SVMs.\Todo{Ref for applications!} 
Formally, for $x_1, x_2 \in \cX$ the kernels are defined as
\begin{align}
	\Gamma(x_1, x_2) &\coloneqq \left(e^{-\frac{\norm{x_1 - x_2}_\cX^2}{s^2}} + r\right) \cdot \mathbbm{1}_2\\
	K(x_1, x_2) &\coloneqq e^{-\frac{\norm{x_1 - x_2}_\cX^2}{s^2}} + r \ .
\end{align}
$\mathbbm{1}_2$ is the $2\times2$-identity matrix and $s, r \in \R$ the kernels' parameters which we set to $s = 5$ and $r = 0.1$ for both.
Note that those kernels are well-defined in the sense that they are functions $\Gamma: \cX \times \cX \rightarrow L(\cX, \cX)$ and $K: \cX \times \cX \rightarrow L(\cY, \cY)$ and satisfy all properties required of them in \cref{def:kernel} (which we will not proof here).

\subsubsection{Remarks on the Implementation}

The implementation of the algorithm used in the experiments is available at \url{https://github.com/NKlug/thesis-mechanical-regression}.

Write:
\begin{itemize}
	\item Tikhonov regularization was necessary.
	\item BlockOperator notation -> we worked in the isomorphic matrix ring to simplify computations.
\end{itemize}
Tikhonov regularization was necessary!


\subsubsection{Results}

Here will be a figure comparing the flows with different parameters.

