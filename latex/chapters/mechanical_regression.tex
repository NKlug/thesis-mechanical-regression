\section{Mechanical Regression}

\Todo{Include figure of typical ResNet Layout}
Artificial Neural Networks are a popular method to solve the Supervised Learning problem described in Section X.
Especially Residual Neural Networks have shown great success in enabling very deep models.
As described earlier, RNNs usually consist of several Residual Blocks followed by dense mapping to the target space $Y$.
This section essentially follows Chapter 3 in \cite{owhadi20} and aims to provide a profound and comprehensive explanation of Mechanical Regression and how it can be interpreted as the continuous limit of ANNs, concretely RNNs.



We will now take a similar approach and try to approximate the target function $f^\dagger$ by a function
\begin{equation}
	f^\ast \coloneqq f \circ \Phi_L \ ,
\end{equation}
where 
\begin{equation}
	\Phi_L \coloneqq \phi_L \circ \phi_{L-1} \circ \ldots \circ \phi_1
\end{equation} 
is the composition of $L$ Residual Blocks, that is, functions $\phi_k = I + v_k$.
Here, the $v_k$ are functions mapping the input space $\cX$ onto itself and $I$ is the identity operator.
Thus, we can regard $\Phi_L$ as a large deformation of $\cX$.
$f: \cX \to \cY$ is a function mapping the deformed space to the target space $\cY$.

By default, the $v_k$ and $f$ can be arbitrary functions.
\Todo{ref: chapter on regularization in Deep Learning? At least explain further!}
Thus, as common in Machine Learning, in order to avoid overfitting the functions to the data, we penalize $v_k$s and $f$s with large norms.
For that, we introduce two RKHSs:
$\mathcal{V} \subseteq \{\cX \rightarrow \cX\}$ and $\mathcal{H} \subseteq \{\cX \rightarrow \cY\}$. 
By \cref{theo:kernel-for-rkhs} there is a Kernel associated with each RKHS.
Let $\Gamma$ be the Kernel associated with $\mathcal{V}$ and $K$ that of $\mathcal{H}$.
We then can identify $f$ and the $v_k$ as solutions to the following problem:
\begin{equation}
	\label{eq:min-v-f}
	\begin{cases}
		\text{Minimize~} & \nu \cdot \frac{L}{2} \sum_{k=1}^{L} \norm{v_k}_\mathcal{V}^2
		+ \lambda \norm{f}_\mathcal{H}^2 
		+ l((f \circ \Phi_L)(X), Y) \\
		\text{such that~} & v_1, \ldots, v_L \in \mathcal{V}, f \in \mathcal{H} \ .
	\end{cases}
\end{equation}
Here, $\nu$ and $\lambda$ are strictly positive balancing parameters.
$l$ is a (positive) loss measuring the similarity of the predicted outputs, that is, the image of $X$ under $f \circ \Phi_L$.

Utilizing the Ridge Regression Loss $l_R$ (\cref{eq:ridge-regression-loss}), we can rewrite the above minimization problem as
\begin{equation}
	\begin{cases}
		\text{Minimize~} & \nu \cdot \frac{L}{2} \sum_{k=1}^{L} \norm{v_k}_\mathcal{V}^2
		+ l_R(\Phi_L(X), Y) \\
		\text{such that~} & v_1, \ldots, v_L \in \mathcal{V}\ .
	\end{cases}
\end{equation}
By "hiding" the regularity of $f$ in the loss we can for now focus on the functions $v_k$.
For our calculations we will therefore only assume that $l: \cX^N \times \cY^N \rightarrow$ is a positive, continuous loss function.
If desired we can later balance the $v_k$ with the regularity of $f$ by choosing the loss appropriately.

Our main goal will now be to show that the minimization problem in \cref{eq:min-v-f} can in fact be considered as a discrete solver of a mechanical system.
In order to do that, we first reformulate the problem by introducing additional variables $q_{i,j}$ with $2 \leq i \leq L+1$, $1 \leq j \leq N$:
\begin{align}
	q_{1, j} &\coloneqq X_j \, \\
	q_{i, j} &\coloneqq (\phi_{i-1} \circ \ldots \circ \phi_1) (X_j) \ .
\end{align}
Write $q_i$ for the vector with entries $q_{i,j}$ and hence we get $q_1 = X$ and $q_i = \phi_i(q_{i-1})$.
Regard the following minimization problem:
\begin{equation}
	\label{eq:min-q}
	\begin{cases}
		\text{Minimize~} & \nu \cdot \frac{1}{2} \sum_{k=1}^{L} \frac{(q_{k+1} - q_k)^\mathrm{T}}{\Delta t} \mathbf{\Gamma}(q_k, q_k)^{-1} \left(\frac{q_{k+1} - q_k}{\Delta t}\right) \Delta t+ l(q_{L+1}, Y) \\
		\text{such that~} & q_1 = X \text{~and~} q_2, \ldots, q_{L+1} \in \cX^N \ .
	\end{cases}
\end{equation}
We will now prove that \cref{eq:min-q} is in fact equivalent to \cref{eq:min-v-f} and, assuming we know the solution to the latter problem, provides us with an explicit solution for the $v_k$s.

\begin{theorem}
	$v_1, \ldots, v_L \in \mathcal{V}$ minimize \cref{eq:min-v-f} 
	$\Leftrightarrow$
	$q_1, \ldots, q_{L+1} \in \cX^N$ minimize \cref{eq:min-q} and
	$v_k(x) = \mathbf{\Gamma}(x, q_k)^\mathrm{T}\mathbf{\Gamma}(q_k, q_k)^{-1} (q_{k+1} - q_k)$.
\end{theorem}
\begin{proof}
	Using the above defined $q_k$ we have $v_k(q_k) = q_{k+1} - q_k$ for all $k \in [L]$.
	With that we can rewrite \cref{eq:min-v-f} as
	\begin{equation}
		\label{eq:min-q-v}
		\begin{cases}
			\text{Minimize~} & \nu \cdot \frac{L}{2} \sum_{k=1}^{L} \norm{v_k}_\mathcal{V}^2
			+ l(q_{L+1}, Y) \\
			\text{such that~} & v_1, \ldots, v_L \in \mathcal{V},\ \forall k \in [L]: v_k(q_k) = q_{k+1} - q_k, \\
			& q_1 = X \text{~and~} q_2, \ldots, q_{L+1} \in \cX^N \ .
		\end{cases}
	\end{equation}
	Here we just added the $q_k$ as additional variables but then bound them to certain values, concretely $q_{k+1} = q_k + v_k(q_k) = \phi_k(q_k)$, with $q_1 = X$.
	Recursively we get $q_{L+1} = \Phi_L(X)$ which means the target function remains the same as in \cref{eq:min-v-f}.
	Note that even though we seemingly added restraints to the domains of the $v_k$ they are still the same: We just have to choose $q_{k+1}$ accordingly (which we can, as it is unconstrained).
	
	We will now derive closed form expressions for the $v_k$ as a function of the $q_k$.
	For that, let $q_k \in \cX^N$, $2 \leq k \leq N$ be arbitrary but fixed .
	This means $l(q^{L+1}, Y)$ is constant and $v_k \in V_{k, q} \coloneqq \{v \in \mathcal{V}~|~ v_k(q_k) = q_{k+1} - q_k\}$.
%	These sets remain convex: Let $\lambda \in [0, 1]$ and $v_{k, 1}, v_{k, 2} \in V_{k, q}$, then we get
%	\begin{align}
%		(\lambda v_{k, 1} + (1 - \lambda) v_{k, 2})(q_k) &= (\lambda v_{k, 1})(q_k) + ((1 - \lambda) v_{k, 2})(q_k)\\
%		&=\lambda(q_{k+1} - q_k) + (1 - \lambda)(q_{k+1} - q_k)\\
%		&= q_{k+1} - q_k \ .
%	\end{align}
	We end up with a problem of the following form:
	\begin{equation}
		\begin{cases}
			\text{Minimize} &\sum_{i=1}^n f_i(x_i)\\
			\text{such that} & \forall i \in [n]: x_i \in A_i \ ,
		\end{cases}
	\end{equation}
	with bounded $f_i: A_i \rightarrow \R$.
	One can easily verify that minimization problems of this type have a global minimum at $x_i^\ast = \argmin\{f_i(x_i)~|~ x_i \in A_i\}$, the boundedness ensuring that at least one such minimum exists.

	Thus, we have to find the minima of $\min\{\norm{v_k}_\mathcal{V}^2~|~ v_k \in \mathcal{V}, v_k(q_k) = q_{k+1} - q_k\} = l_{\text{OR}}(q_k, q_{k+1} - q_k)$, $l_{\text{OR}}$ being the optimal recovery loss from \cref{sec:optimal-recovery}.
	From that section we conveniently get the following representations for the minimal $v_k$ and the target value:
	\begin{align}
		v_k & = \mathbf{\Gamma}(x, q_k)^\mathrm{T}\mathbf{\Gamma}(q_k, q_k)^{-1} (q_{k+1} - q_k)\\
		\norm{v_k}_\mathcal{H}^2 &= (q_{k+1} - q_k)^\mathrm{T} \mathbf{\Gamma}(q_k, q_k)^{-1} (q_{k+1} - q_k)
	\end{align}
	Here, $\mathbf{\Gamma}$ is the block operator matrix with entries $\Gamma(q_{k,i}, q_{k, j})$ and $\mathbf{\Gamma}(x, q_k)$ the vector $(\Gamma(x, q_{k, i}))_{i \in [N]}$.
	++
	Having now computed optimal $v_k$ -- or rather their squared $\mathcal{H}$-norms -- as a function of $q_2, \ldots, q_{L+1}$ we can reformulate \cref{eq:min-q-v} without the $v_k$ as variables.
	Define $\Delta t \coloneqq \frac{1}{L}$.
	We get:
	\begin{equation}
		\label{eq:}
		\begin{cases}
			\text{Minimize~} & \nu \cdot \frac{1}{2} \sum_{k=1}^{L}  
			\left(\frac{q_{k+1} - q_k}{\Delta t}\right)^\mathrm{T} \mathbf{\Gamma}(q_k, q_k)
			\left(\frac{q_{k+1} - q_k}{\Delta t}\right) \cdot \Delta t
			+ l(q_{L+1}, Y) \\
			\text{such that~} & q_1 = X \text{~and~} q_2, \ldots, q_{L+1} \in \cX^N \ .
		\end{cases}
	\end{equation}
	As all steps and transformations hold true in both ways, this concludes the proof.
\end{proof}

\subsection{Least Action Principle}

Take a closer look at the first term of the objective function in \cref{eq:min-q} (the parameter $\nu$ has been deliberately omitted here):
\begin{equation}
	\label{eq:discrete-lagrangian}
	\frac{1}{2} \sum_{k=1}^{L} \left(\frac{q_{k+1} - q_k}{\Delta t}\right)^\mathrm{T} \mathbf{\Gamma}(q_k, q_k)^{-1} \frac{q_{k+1} - q_k}{\Delta t} \Delta t \ .
\end{equation}
With some imagination this very much looks like an approximation of the integral of some continuous function with a step function using $L$ intervals of width $\Delta t$.
This would require the sequence $q_k$ to be a discrete equidistant sampling of a function $q: A \rightarrow \cX^N$, where $A$ is an interval in $\R$.
Without loss of generality we choose $A = [0, 1]$ and write $q_k = q(k \Delta t)$.
This way, $\frac{q_{k+1} - q_k}{\Delta t}$ can be interpreted as a forward differential quotient approximating the derivative $\dot{q}$ on at the point $k \Delta t = \frac{k}{L}$ and we get the approximation
\begin{equation}
	\left(\frac{q_{k+1} - q_k}{\Delta t}\right)^\mathrm{T} \mathbf{\Gamma}(q_k, q_k)^{-1} \frac{q_{k+1} - q_k}{\Delta t} 
	\approx \dot{q}\left(\frac{k}{L}\right)^\mathrm{T} \mathbf{\Gamma}\left(q\left(\frac{k}{L}\right), q\left(\frac{k}{L}\right)\right)^{-1}\dot{q}\left(\frac{k}{L}\right)
\end{equation}
This leads us to the obvious question if the solutions of the problem in \cref{eq:min-q} are also approximating the solutions of a similar continuous problem.
As it turns out, they do.
Consider 
\begin{equation}
	\mathcal{A}(q) \coloneqq \int_{0}^{1} \fL(q, \dot{q}, t) \mathrm{d}t \ ,
\end{equation}
with
\begin{equation}
	\fL(q, \dot{q}, t) \coloneqq \frac{1}{2} \dot{q}(t)^\mathrm{T} \mathbf{\Gamma}(q(t), q(t))^{-1} \dot{q}(t) \ .
\end{equation}
We will later prove the following theorem:
\begin{theorem}
	The minimal value of \cref{eq:min-q} converges towards the minimal value of             
	\Todo{Theorem 3.11 from paper} 
	\begin{equation}
	\label{eq:cont-least-action}
		\begin{cases}
			\text{Minimize~} & \nu \mathcal{A}(q) + l(q(1), Y)\\
			\text{such that~} & q \in C^1([0,1], \cX^N),\ q(0) = X \ .
		\end{cases}
	\end{equation}
	$C^1([0,1], \cX^N$ is the set of continuously differentiable functions $q: [0, 1] \rightarrow \cX^N$.
\end{theorem}
\Todo{Überleitung zur Physik}

Assuming a physical point of view, $\cL$ is the \Todo{ref!}\emph{Lagrangian} and $\mathcal{A}$ the \emph{action}.

\subsection{Hamiltonian Representation}

\subsection{Continuous Limit and Adherence Values}