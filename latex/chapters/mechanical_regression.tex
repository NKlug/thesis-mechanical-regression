\section{Mechanical Regression}
\label{sec:mechanical-regression}

In \cref{sec:neural-networks} we have seen that neural networks are a popular and extremely successful way of approximating a solution to the supervised learning problem.
ResNets in particular enabled the use of very deep architectures.
In this section, we take a closer look at a simplified model of ResNets and derive their continuous limit as the number of residual blocks tends towards infinity.
On the way of doing so, we show that the ResNet regression approach can be regarded as a discrete mechanical system following Hamilton's stationary action principle -- thus the term \emph{Mechanical Regression}.
In the limit, this mechanical system converges to a continuous version of the same principle, which proves to be equivalent to what in image registration is known as geodesic shooting \cite{allassonniere05}.

\citet{owhadi20} compares these results to the problem formulations used in image shape analysis, computational anatomy and image registration.
He views the continuous solution as a generalization of image registration and calls it \emph{idea registration}, arguing that just as in the former, data points are aligned through various transformations in their respective feature spaces.
The difference lies in the fact that in idea registration the data points can be arbitrary rather than mere landmarks and the spaces can be high dimensional (compared to the two or three dimensional spaces images lie in).

This section essentially follows Chapter 3 of \cite{owhadi20} and aims to provide a profound and comprehensive explanation of Mechanical Regression and how it can be interpreted as the continuous limit of ResNets.
\cref{fig:convergence} gives an overview about the problems and theorems presented in this section and can be used as a look-up table.

\subsection{Modeling Residual Neural Networks}

Recall the supervised learning problem: Given training data $X$ and $Y$ consisting of $X_i \in \cX$, $Y_i \in \cY$ and $f^\dagger(X) = Y$, approximate $f^\dagger$.
On possibility to solve this problem are residual neural networks, which approximate the target function $f^\dagger$ through a series of residual blocks.
In mathematical terms we can write
\begin{equation}
	f^\ast \coloneqq f \circ \Phi_L 
\end{equation}
for the approximate solution, where
\begin{equation}
\Phi_L \coloneqq \phi_L \circ \phi_{L-1} \circ \ldots \circ \phi_1.
\end{equation} 
$\Phi_L$ is the composition of $L$ residual blocks, that is, functions $\phi_k = I + v_k$.
Here, the $v_k$ are functions mapping the input space $\cX$ onto itself and $I$ is the identity operator.
Thus, we can regard $\Phi_L$ as a large deformation of $\cX$.
$f: \cX \to \cY$ is a function mapping the deformed space to the target space $\cY$.
Note that this model is but a simplified model of ResNets.
In reality, the residual blocks often map the input space to a different space, as is the case in most convolutional networks \cite{he16}.

By default, the $v_k$ and $f$ can be arbitrary functions.
However, it poses a challenge to not only approximate the target with any series of functions, but with such that generalize well, meaning that they also perform well on data other than the training data $(X, Y)$.
Thus, it is common in machine learning \cite{goodfellow16} to apply some form of regularization.
One popular approach, which we will also use here, is to apply penalties to the parameters' norms -- which are, in this case, the norms of $v_k$ and $f$.

We still need to define appropriate normed spaces for the functions $v_k$ and $f$.
For that, we introduce two RKHSs
$\cV \subseteq \{\cX \rightarrow \cX\}$ and $\cH \subseteq \{\cX \rightarrow \cY\}$ and restrict the $v_k$ and $f$ to lie in these spaces, respectively.
By \cref{theo:kernel-for-rkhs} there is a Kernel associated with each RKHS.
Let $\Gamma$ be the Kernel associated with $\cV$ and $K$ that of $\cH$.
Then we can identify $f$ and the $v_k$ as solutions to the following problem:
\begin{problem}
	\label{prob:min-v-f}
	\begin{cases}
		\text{Minimize~} & \nu \cdot \frac{L}{2} \sum_{k=1}^{L} \norm{v_k}_\cV^2
		+ \lambda \norm{f}_\cH^2 
		+ l((f \circ \Phi_L)(X), Y) \\
		\text{such that~} & v_1, \ldots, v_L \in \cV, f \in \cH \ .
	\end{cases}
\end{problem}
Here, $\nu$ and $\lambda$ are strictly positive balancing parameters.
$l$ is a (positive) loss measuring the similarity of the predicted outputs, that is, the image of $X$ under $f \circ \Phi_L$.
Just as intended, $v_k$ and $f$ with large norms are penalized.

Utilizing the ridge regression loss $l_R$ (\cref{eq:ridge-regression-loss}), we can rewrite the above minimization problem as
\begin{problem}
	\begin{cases}
		\text{Minimize~} & \nu \cdot \frac{L}{2} \sum_{k=1}^{L} \norm{v_k}_\cV^2
		+ l_R(\Phi_L(X), Y) \\
		\text{such that~} & v_1, \ldots, v_L \in \cV\ .
	\end{cases}
\end{problem}
By hiding the regularity of $f$ in the loss we can, for now, focus exclusively on the functions $v_k$.
For our calculations we will only assume that $l$ is a positive, continuous loss function.
If desired we can later balance the $v_k$ with the regularity of $f$ by choosing the loss appropriately.

In the calculations, we will work under the following conditions.
\begin{condition}
	\label{cond:feature-condition}\mbox{}
	\vspace*{-\parsep}
	\vspace*{-\baselineskip}
	\begin{enumerate}
		\item There exist a finite dimensional feature space and map $\cF$ and $\psi$ such that $\psi$ and its first and second order partial derivatives are continuous and uniformly bounded.
		\item There exists an $r > 0$ such that $\forall Z \in \cX^N:~Z^\T \bGamma(X, X) Z \geq r Z^\T Z$.
		\item $\cX$ and $\cY$ are finite-dimensional.
		\item $l: \cX^N \times \cY^N \rightarrow$ is a positive and continuous loss.
	\end{enumerate}
\end{condition}
Just like in \cref{sec:optimal-recovery}, $\bGamma(X, X)$ is the block operator matrix with entries $\Gamma(X_i, X_j)$.
Note that the first condition implies that the function $(x_1, x_2) \rightarrow \Gamma(x_1, x_2)$ and its first and second order partial derivatives are continuous and uniformly bounded.
This follows immediately from the fact that we can write $\Gamma(x_1, x_2) = \psi^\T(x_1)\psi(x_2)$.
Furthermore, we have the following equivalence for \cref{cond:feature-condition} (2).
\begin{lemma}
	There exists $r > 0$ such that $\forall Z \in \cX^N:~Z^\T \bGamma(X, X) Z \geq r Z^\T Z$ if and only if $\bGamma(X, X)$ is non-singular.
\end{lemma}
\begin{proof}
	If $\bGamma(X, X)$ was singular, there would exist a vector $Z \in \cX^N$ with $\norm{Z}_{\cX^N} > 0$ and $\bGamma(X, X) Z = 0$, implying $Z^\T \bGamma(X, X)Z = 0$. 
	A contradiction.
	Conversely, if there exists a $0 \neq Z \in \cX^N$ such that $Z^\T \bGamma(X, X) Z < \epsilon Z^\T Z$ for all $\epsilon > 0$, then $\bGamma(X, X) Z = 0$.
\end{proof}
Ideally, we would not need additional conditions like those introduced above.
They are, however, automatically satisfied in most real world examples and ease calculations and proofs considerably.

\subsection{Discrete Stationary Action Principle}

\input{chapters/discrete_least_action_principle}

\subsection{Continuous Stationary Action Principle}

\input{chapters/least_action_principle}

\subsection{Hamiltonian Representation}

\input{chapters/hamiltonian_representation}

\subsection{Geodesic Shooting}

Using the Hamiltonian formulation, we can now formulate another problem equivalent to \cref{prob:cont-least-action}.
The search for the optimal trajectory $q$ is reduced to the search for the optimal initial momentum $p(0)$.
A similar method, called \emph{Geodesic Shooting} has been introduced by \citet{allassonniere05} in the field of image registration.
In this method, an optimal control problem is also reduced to finding optimal initial momenta for a Hamiltonian system.
For mechanical regression, the equivalent problem is given as follows.
\begin{problem}
	\label{prob:geodesic-shooting}
	\begin{cases}
		\text{Minimize~}& \frac{\nu}{2} p(0)^\T \bGamma(X, X)p(0) + l(q(1), Y)\\
		\text{such that~} & p = \bGamma(q, q)^{-1}\dot{q},\ q(0) = X \\
		&\text{and~} (q,p) \text{~follow Hamilton's equations \ref{eq:hamiltonian-system}} \ .
	\end{cases}
\end{problem}
For later use, we explicitly define the objective function of this problem:
\begin{equation}
\label{eq:geodesic-shooting-objective}
\fV(p(0), X, Y) \coloneqq \frac{\nu}{2} p(0)^\T \bGamma(X, X)p(0) + l(q(1), Y) \ .
\end{equation}

The following theorem states the equivalence between \cref{prob:cont-least-action} and \cref{prob:geodesic-shooting}, which means that the Least Action formulation can be solved by Geodesic Shooting.
\begin{theorem}
	\label{theo:geodesic-shooting}
	$q \in C^1([0, 1], \cX^N)$ minimizes \cref{prob:cont-least-action} if and only if with $p\coloneqq \bGamma(q, q)^{-1}\dot{q}$, $p(0)$ minimizes \cref{prob:geodesic-shooting}.
\end{theorem}
\begin{proof}
	Let $q$ minimize \cref{prob:cont-least-action} and define $p = \bGamma(q, q)^{-1}\dot{q}$.
	Then $q(0) = X$ and by \cref{theo:hamiltonian-dynamic} $(q, p)$ satisfy Hamilton's equations \ref{eq:hamiltonian-system}.
	We have $\cL(t, q(t), \dot{q}(t)) = \cH(t, q(t), p(t))$ and know from \cref{cor:energy-preservation} that $\cH$ is constant across time, that is for $t_1, t_2 \in [0, 1]$ $\cH(t_1, q(t_1), p(t_1)) = \cH(t_2, q(t_2), p(t_2))$.
	This implies that
	\begin{equation}
	\mathcal{A}(q) = \int_{0}^{1} \cL(t, q(t), \dot{q}(t)) \mathrm{d}t 
	= \int_{0}^{1} \cH(t, q(t), p(t)) \mathrm{d}t = \cH(t_1, q, p) \ .
	\end{equation}
	Therefore $(q, p)$ minimize $\nu \cH(t_1, q(t_1), p(t_1) + l(q(1), Y)$ and for $t_1 = 0$ we get
	\begin{align}
	\nu \cH(0, q(0), p(0)) + l(q(1), Y) &= \frac{\nu}{2} p(0)^\T \bGamma(q(0), q(0))p(0) + l(q(1), Y)\\
	&= \frac{\nu}{2} p(0)^\T \bGamma(X, X)p(0) + l(q(1), Y) \ ,
	\end{align}
	which is the target function $\fV$ defined in \cref{prob:geodesic-shooting}.
	Vice versa, if $p(0)$ minimizes $\fV$ and $(q, p)$ follow Hamilton's equations, with the same argument, $q$ minimizes \cref{prob:cont-least-action}.
\end{proof}

\input{chapters/continuous_limit}