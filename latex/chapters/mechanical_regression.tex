\section{Mechanical Regression}

\Todo{Include figure of typical ResNet Layout}
Artificial Neural Networks are a popular method to solve the Supervised Learning problem described in Section X.
Especially Residual Neural Networks have shown great success in enabling very deep models.
As described earlier, RNNs usually consist of several Residual Blocks followed by dense mapping to the target space $Y$.
This section essentially follows Chapter 3 in \cite{owhadi20} and aims to provide a profound and comprehensive explanation of Mechanical Regression and how it can be interpreted as the continuous limit of ANNs, concretely RNNs.

\begin{condition}
	\label{cond:feature-condition}
	\begin{enumerate}
		\item There exist a finite dimensional feature space and map $\cF$ and $\psi$ such that $\psi$ and its first and second order partial derivatives are continuous and uniformly bounded
	\end{enumerate}
\end{condition}

\subsection{Modeling Residual Neural Networks}
We will now take a similar approach and try to approximate the target function $f^\dagger$ by a function
\begin{equation}
	f^\ast \coloneqq f \circ \Phi_L \ ,
\end{equation}
where 
\begin{equation}
	\Phi_L \coloneqq \phi_L \circ \phi_{L-1} \circ \ldots \circ \phi_1
\end{equation} 
is the composition of $L$ Residual Blocks, that is, functions $\phi_k = I + v_k$.
Here, the $v_k$ are functions mapping the input space $\cX$ onto itself and $I$ is the identity operator.
Thus, we can regard $\Phi_L$ as a large deformation of $\cX$.
$f: \cX \to \cY$ is a function mapping the deformed space to the target space $\cY$.

By default, the $v_k$ and $f$ can be arbitrary functions.
\Todo{ref: chapter on regularization in Deep Learning? At least explain further!}
Thus, as common in Machine Learning, in order to avoid overfitting the functions to the data, we penalize $v_k$s and $f$s with large norms.
For that, we introduce two RKHSs:
$\cV \subseteq \{\cX \rightarrow \cX\}$ and $\cH \subseteq \{\cX \rightarrow \cY\}$. 
By \cref{theo:kernel-for-rkhs} there is a Kernel associated with each RKHS.
Let $\Gamma$ be the Kernel associated with $\cV$ and $K$ that of $\cH$.
We then can identify $f$ and the $v_k$ as solutions to the following problem:
\begin{problem}
	\label{prob:min-v-f}
	\begin{cases}
		\text{Minimize~} & \nu \cdot \frac{L}{2} \sum_{k=1}^{L} \norm{v_k}_\cV^2
		+ \lambda \norm{f}_\cH^2 
		+ l((f \circ \Phi_L)(X), Y) \\
		\text{such that~} & v_1, \ldots, v_L \in \cV, f \in \cH \ .
	\end{cases}
\end{problem}
Here, $\nu$ and $\lambda$ are strictly positive balancing parameters.
$l$ is a (positive) loss measuring the similarity of the predicted outputs, that is, the image of $X$ under $f \circ \Phi_L$.

Utilizing the Ridge Regression Loss $l_R$ (\cref{eq:ridge-regression-loss}), we can rewrite the above minimization problem as
\begin{problem}
	\begin{cases}
		\text{Minimize~} & \nu \cdot \frac{L}{2} \sum_{k=1}^{L} \norm{v_k}_\cV^2
		+ l_R(\Phi_L(X), Y) \\
		\text{such that~} & v_1, \ldots, v_L \in \cV\ .
	\end{cases}
\end{problem}
By "hiding" the regularity of $f$ in the loss we can for now focus on the functions $v_k$.
For our calculations we will therefore only assume that $l: \cX^N \times \cY^N \rightarrow$ is a positive, continuous loss function.
If desired we can later balance the $v_k$ with the regularity of $f$ by choosing the loss appropriately.

Our main goal will now be to show that the minimization problem in \cref{prob:min-v-f} can in fact be considered as a discrete solver of a mechanical system.
In order to do that, we first reformulate the problem by introducing additional variables $q_{i,j}$ with $2 \leq i \leq L+1$, $1 \leq j \leq N$:
\begin{align}
	q_{1, j} &\coloneqq X_j \, \\
	q_{i, j} &\coloneqq (\phi_{i-1} \circ \ldots \circ \phi_1) (X_j) \ .
\end{align}
Write $q_i$ for the vector with entries $q_{i,j}$ and hence we get $q_1 = X$ and $q_i = \phi_i(q_{i-1})$.
Regard the following minimization problem:
\begin{problem}
	\label{prob:min-q}
	\begin{cases}
		\text{Minimize~} & \nu \cdot \frac{1}{2} \sum_{k=1}^{L} \frac{(q_{k+1} - q_k)^\mathrm{T}}{\Delta t} \mathbf{\Gamma}(q_k, q_k)^{-1} \left(\frac{q_{k+1} - q_k}{\Delta t}\right) \Delta t+ l(q_{L+1}, Y) \\
		\text{such that~} & q_1 = X \text{~and~} q_2, \ldots, q_{L+1} \in \cX^N \ .
	\end{cases}
\end{problem}
We will now prove that \cref{prob:min-q} is in fact equivalent to \cref{prob:min-v-f} and, assuming we know the solution to the latter problem, provides us with an explicit solution for the $v_k$s.

\begin{theorem}
	$v_1, \ldots, v_L \in \cV$ minimize \cref{prob:min-v-f} 
	$\Leftrightarrow$
	$q_1, \ldots, q_{L+1} \in \cX^N$ minimize \cref{prob:min-q} and
	$v_k(x) = \mathbf{\Gamma}(x, q_k)^\mathrm{T}\mathbf{\Gamma}(q_k, q_k)^{-1} (q_{k+1} - q_k)$.
\end{theorem}
\begin{proof}
	Using the above defined $q_k$ we have $v_k(q_k) = q_{k+1} - q_k$ for all $k \in [L]$.
	With that we can rewrite \cref{prob:min-v-f} as
	\begin{problem}
		\label{prob:min-q-v}
		\begin{cases}
			\text{Minimize~} & \nu \cdot \frac{L}{2} \sum_{k=1}^{L} \norm{v_k}_\cV^2
			+ l(q_{L+1}, Y) \\
			\text{such that~} & v_1, \ldots, v_L \in \cV,\ \forall k \in [L]: v_k(q_k) = q_{k+1} - q_k, \\
			& q_1 = X \text{~and~} q_2, \ldots, q_{L+1} \in \cX^N \ .
		\end{cases}
	\end{problem}
	Here we just added the $q_k$ as additional variables but then bound them to certain values, concretely $q_{k+1} = q_k + v_k(q_k) = \phi_k(q_k)$, with $q_1 = X$.
	Recursively we get $q_{L+1} = \Phi_L(X)$ which means the target function remains the same as in \cref{prob:min-v-f}.
	Note that even though we seemingly added restraints to the domains of the $v_k$ they are still the same: We just have to choose $q_{k+1}$ accordingly (which we can, as it is unconstrained).
	
	We will now derive closed form expressions for the $v_k$ as a function of the $q_k$.
	For that, let $q_k \in \cX^N$, $2 \leq k \leq N$ be arbitrary but fixed .
	This means $l(q^{L+1}, Y)$ is constant and $v_k \in V_{k, q} \coloneqq \{v \in \cV~|~ v_k(q_k) = q_{k+1} - q_k\}$.
%	These sets remain convex: Let $\lambda \in [0, 1]$ and $v_{k, 1}, v_{k, 2} \in V_{k, q}$, then we get
%	\begin{align}
%		(\lambda v_{k, 1} + (1 - \lambda) v_{k, 2})(q_k) &= (\lambda v_{k, 1})(q_k) + ((1 - \lambda) v_{k, 2})(q_k)\\
%		&=\lambda(q_{k+1} - q_k) + (1 - \lambda)(q_{k+1} - q_k)\\
%		&= q_{k+1} - q_k \ .
%	\end{align}
	We end up with a problem of the following form:
	\begin{problem}
		\begin{cases}
			\text{Minimize} &\sum_{i=1}^n f_i(x_i)\\
			\text{such that} & \forall i \in [n]: x_i \in A_i \ ,
		\end{cases}
	\end{problem}
	with bounded $f_i: A_i \rightarrow \R$.
	One can easily verify that minimization problems of this type have a global minimum at $x_i^\ast = \argmin\{f_i(x_i)~|~ x_i \in A_i\}$, the boundedness ensuring that at least one such minimum exists.

	Thus, we have to find the minima of $\min\{\norm{v_k}_\cV^2~|~ v_k \in \cV, v_k(q_k) = q_{k+1} - q_k\} = l_{\text{OR}}(q_k, q_{k+1} - q_k)$, $l_{\text{OR}}$ being the optimal recovery loss from \cref{sec:optimal-recovery}.
	From that section we conveniently get the following representations for the minimal $v_k$ and the target value:
	\begin{align}
		v_k & = \mathbf{\Gamma}(x, q_k)^\mathrm{T}\mathbf{\Gamma}(q_k, q_k)^{-1} (q_{k+1} - q_k)\\
		\norm{v_k}_\cH^2 &= (q_{k+1} - q_k)^\mathrm{T} \mathbf{\Gamma}(q_k, q_k)^{-1} (q_{k+1} - q_k)
	\end{align}
	Here, $\mathbf{\Gamma}$ is the block operator matrix with entries $\Gamma(q_{k,i}, q_{k, j})$ and $\mathbf{\Gamma}(x, q_k)$ the vector $(\Gamma(x, q_{k, i}))_{i \in [N]}$.
	++
	Having now computed optimal $v_k$ -- or rather their squared $\cH$-norms -- as a function of $q_2, \ldots, q_{L+1}$ we can reformulate \cref{prob:min-q-v} without the $v_k$ as variables.
	Define $\Delta t \coloneqq \frac{1}{L}$.
	We get:
	\begin{problem}
		\begin{cases}
			\text{Minimize~} & \nu \cdot \frac{1}{2} \sum_{k=1}^{L}  
			\left(\frac{q_{k+1} - q_k}{\Delta t}\right)^\mathrm{T} \mathbf{\Gamma}(q_k, q_k)
			\left(\frac{q_{k+1} - q_k}{\Delta t}\right) \cdot \Delta t
			+ l(q_{L+1}, Y) \\
			\text{such that~} & q_1 = X \text{~and~} q_2, \ldots, q_{L+1} \in \cX^N \ .
		\end{cases}
	\end{problem}
	As all steps and transformations hold true in both ways, this concludes the proof.
\end{proof}

\subsection{Least Action Principle}

\input{chapters/least_action_principle}

\subsection{Hamiltonian Representation}

An alternative formulation of Lagrangian mechanics is the Hamiltonian formalism.
In its own, it does not add anything particularly new but rather gives us a more powerful framework to work with the already established theory.
In essence, a change of variables from $(q, \dot{q}, t)$ to $(q, p, t)$ is applied through a certain kind of transformation called \emph{Legendre transformation}.
The obtained $(q, p)$ are known as the \emph{canonical variables}, concretely canonical coordinate and canonical momentum.
We will briefly derive the Hamiltonian formulation for our application, but not cover all the theoretical details.
For a complete formal derivation from a physical point of view, see \cite[Chapter~8]{goldstein01}.
For a more mathematical approach, see \cite[Chapter~2]{marsden10}.

First, define the canonical momentum
\begin{equation}
	p(t) \coloneqq \frac{\partial \fL(q, \dot{q}, t)}{\partial \dot{q}} = \mathbf{\Gamma}(q(t), q(t))^{-1} \dot{q}(t) \ .
\end{equation}
Note that just as we actually had $N$ Euler-Lagrange Equations above, we now also have $N$ canonical momenta, each defined as $p_i = \frac{\partial \fL}{\partial \dot{q}_i}$.
Again, we use vector notation, and call $p$ \emph{the} canonical momentum.
Next, we define the \emph{Hamiltonian function}, often also called \emph{energy function} \cite{marsden10}:
\begin{equation}
	\fH(q, p, t) \coloneqq p(t)^\mathrm{T}\dot{q}(t) - \fL(q, \dot{q}, t) \ .
\end{equation}
In physical context, $\fH$ can often be expressed as the sum of the system's kinetic and potential energies.
Hence $\fH$ is a measure of the total energy of the system.
Here, we get
\begin{align}
	\fH(q, p, t) &= \frac{1}{2} \dot{q}(t)^\T \mathbf{\Gamma}(q(t), q(t))^{-1} \dot{q(t)} \\
	&= \frac{1}{2} \dot{q}(t)^\T \mathbf{\Gamma}(q(t), q(t))^{-1} \mathbf{\Gamma}(q(t), q(t)) \mathbf{\Gamma}(q(t), q(t))^{-1} \dot{q}(t)\\
	&= \frac{1}{2}  \left(\mathbf{\Gamma}(q(t), q(t))^{-1} \dot{q}(t)\right)^\T \mathbf{\Gamma}(q(t), q(t)) \mathbf{\Gamma}(q(t), q(t))^{-1} \dot{q}(t)\\
	&= \frac{1}{2} p(t)^\T \mathbf{\Gamma}(q(t), q(t)) p(t) \ .
\end{align}

As you can see, the equations already get quite cluttered.
Thus, in the following we will, for simplicity, often omit the time dependence of $p$ and $q$.
For example, we write $\Gamma(q, q)$ instead of $\Gamma(q(t), q(t))$.

The next theorem describes the correspondence between Lagrangian and Hamiltonian mechanics:
\Todo{Important: Explain meaning of partial symbol (is to be read as a gradient) or write differently}
\begin{theorem}
	If $q$ minimizes \cref{prob:cont-least-action}, $(q, p)$ follow Hamilton's equations
	\begin{equation}
	\label{eq:hamiltonian-system}
		\begin{split}
				\dot{q} &= \frac{\partial \fH(q, p)}{\partial p} = \mathbf{\Gamma}(q, q) p\\
			\dot{p} &= -\frac{\partial \fH(q, p)}{\partial q} 
			= -\frac{\partial \left(\frac{1}{2} p^\mathrm{T} \mathbf{\Gamma}(q, q) p\right)}{\partial q}
		\end{split}
	\end{equation}
	with $q(0) = X$.
\end{theorem}
\begin{proof}
	Let $q$ be a minimizer of \cref{prob:cont-least-action}.
	Then $q(0) = X$ and, as we have seen, $q$ fulfills the Euler-Lagrange equations.
	The claim immediately follows from the equivalence of the Euler-Lagrange equations and Hamilton's equations, as can be seen in e.g. \cite{marsden10, goldstein01}.
\end{proof}

\Todo{Elaborate!}
Just as before, $q(1)$ was unknown, now, $p(0)$ is to be determined.
This means we our problem now reduces to the search for the initial momentum $p(0)$.


From $\frac{\partial \fL}{\partial t} = 0$ it follows that:
\Todo{Think about whether to keep this. Seems kind of irrelevant}
\begin{corollary}
	\label{cor:energy-preservation}
	Along $q$ the energy is preserved.
\end{corollary}
\begin{proof}
	The Lagrangian is not explicitly time dependent, that is $\frac{\partial \fL}{\partial t} = 0$.
	Therefore
	\begin{equation}
		\frac{\mathrm{d}}{\mathrm{d} t} \fL = \sum_{i=1}^{N}\left(\frac{\partial \fL}{\partial q_i} \dot{q_i} + \frac{\partial \fL}{\partial \dot{q_i}} \ddot{q_i} \right )
		=\sum_{i=1}^{N}\left( \left( \frac{\mathrm{d}}{\mathrm{d} t} \frac{\partial \fL}{\partial \dot{q_i}} 
		\right)\dot{q_i}+ \frac{\partial \fL}{\partial \dot{q_i}} \ddot{q_i}\right )
		= \frac{\mathrm{d}}{\mathrm{d} t} \left(\sum_{i=1}^{N} \frac{\partial \fL}{\partial \dot{q_i}} \dot{q_i}\right) \ .
	\end{equation}
	It follows that
	\begin{equation}
		0 = \frac{\mathrm{d}}{\mathrm{d} t} \left( \left(\sum_{i=1}^{N} \frac{\partial \fL}{\partial \dot{q_i}} \dot{q_i} \right) -  \fL \right) 
		= \frac{\mathrm{d}}{\mathrm{d} t} \left( \left(\sum_{i=1}^{N} p_i \dot{q_i} \right ) - \fL \right)
		= \frac{\mathrm{d}}{\mathrm{d} t} \fH \ ,
	\end{equation}
	which means that the energy function does not change over time and hence is constant.
\end{proof}

Our next task will be to show that there exists a unique solution to the system in \cref{eq:hamiltonian-system}.
If this is the case, we could solve -- or at least approximate -- the flow of the Hamiltonian system to acquire a solution to \cref{prob:cont-least-action}:
A minimizer of \cref{prob:cont-least-action} solves the Hamiltonian system and if that system has a unique solution, this must be it.
Of course, we can only identify the flow once we have determined the optimal $q(1)$ as otherwise we would be lacking an initial value.

\begin{theorem}
	There exists a unique solution $(q, p)$ for the Hamiltonian system in \cref{eq:hamiltonian-system} with $q \in C^2([0, 1], \cX^N)$ and $p \in C^1([0, 1], \cX^N)$.
\end{theorem}

\begin{proof}
	Recall \cref{eq:kernel-feature-map}, which reads $\Gamma(x_1, x_2) = \psi^\mathrm{T}(x_1)\psi(x_2)$.
	Using this equality, we can rewrite \cref{eq:hamiltonian-system} as
	\begin{equation}
	\label{eq:feature-hamiltonian}
		\begin{split}
			\dot{q}_i &= \psi^\mathrm{T}(q_i) \alpha\\
			\dot{p}_i &= -\frac{\partial}{\partial q_i} \left(p_i^\mathrm{T} \psi^\mathrm{T}(q_i) \alpha \right)\ ,
		\end{split}
	\end{equation}
	with $\alpha \coloneqq \sum_{k=1}^{N} \psi(q_k) p_k$.
	From \cref{cor:feature-space-norm} we get:
	\begin{align}
		\norm{\alpha}_\cF^2 &= \norm{\psi^\T \alpha}_\cV^2\\
		&= \norm{\sum_{i=1}^N \psi^\T(x) \psi(q_i)p_i}_\cV^2 \\
		&= \norm{\sum_{i=1}^N \Gamma(x, q_i) p_i}_\cV^2\\
		&= \left<\sum_{i=1}^N\Gamma(x, q_i)p_i, \sum_{j=1}^N \Gamma(x, q_j) p_j \right>_\cV\\
		&= \sum_{i=1}^N \sum_{j=1}^N \left<\Gamma(x, q_i)p_i, \Gamma(x, q_j) p_j \right>_\cV\\
		&= \sum_{i=1}^N \sum_{j=1}^N \left<p_i, \Gamma(q_i, q_j) p_j\right>_\cX\\
		&= p^\T \mathbf{\Gamma}(q, q) p \ .
	\end{align}
	In the second to last step we again used the reproducing property of the kernel $\Gamma$.
	This gives us $\norm{\alpha}_\cF^2 = 2 \fH(q, p)$.
	\cref{cor:energy-preservation} states that $\fH$ is constant across time, which implies that $\norm{\alpha}_\cF^2$ is, too (and thus  $\norm{\alpha}_\cF$).
	Our goal is to use a global version of the Picard-Lindelöf theorem \cite[~Theorem 1.2.3]{arino06} to prove that the Hamiltonian system does have a unique solution.
	For this, we need the vector field $(q, p)$ to be globally Lipschitz continuous.
	Showing component-wise Lipschitz continuity suffices because we can choose the global constant $L$ as the maximum of the components'.
	We will show the Lipschitz property by proofing that $\dot{q}$ and $\dot{p}$ are bounded.
	\Todo{Is this bogus math? This looks like a system of PDEs, is Picard-Lindelöf even applicable? Not and yes!
	The derivative wrt time of (q, p) is the componentwise derivative!}
	This will do:
	Let $\dot{q}$ and $\dot{p}$ be bounded by $L \in \R$ and $q_1, p_1, q_2, p_2 \in \cX$, $x \coloneqq (q_1, p_1)^\T, y \coloneqq (q_2, p_2)^\T$ and
	\begin{equation}
	\label{eq:hamiltonian-time-derivative}
		\begin{pmatrix}
			\dot{q}\\\dot{p}
		\end{pmatrix}
		= \frac{\mathrm{d}}{\mathrm{d} t}\begin{pmatrix}q\\p\end{pmatrix}
		= f\left(t, \begin{pmatrix}q\\p\end{pmatrix}\right) 
		\coloneqq 			
		\begin{pmatrix}\mathbf{\Gamma}(q, q)p\\ 
		-\frac{\partial \left(\frac{1}{2} p^\mathrm{T} \mathbf{\Gamma}(q, q) p\right)}{\partial q}
		\end{pmatrix} 
	\end{equation}
	Then by the mean value theorem there exists a point $z \coloneqq (q^\ast, p^\ast)$ with $q^\ast,\ p^\ast \in \cX$ such that
	\begin{align}
		\norm{f\left(t, x\right) - f\left(t, y\right) }
		= \norm{
			\frac{\mathrm{d}}{\mathrm{d} t}f\left(t, z \right)}
		\cdot \norm{x-y} \ .
	\end{align}
	Because of the equality in \cref{eq:hamiltonian-time-derivative} we immediately arrive at the desired result:
	$\norm{f\left(t, x\right) - f\left(t, y\right) } \leq L \norm{x - y}$.
	For the detailed requirements of the Picard-Lindelöf theorem the reader is referred to \cite{arino06, tenenbaum85}.
	
	All that is left to show now is the boundedness of $\dot{q}$ and $\dot{p}$.
	That of of $\dot{q}$ is easy to see from \cref{eq:feature-hamiltonian}:
	\begin{equation}
		\norm{\dot{q}_i}_\cX \leq \norm{\psi^T(q_i)}_O \norm{\alpha}_\cF 
		\leq \sup_{x \in \cX}\norm{\psi^T(x)}_O \norm{\alpha}_\cF\ .
	\end{equation}
	Here, $\norm{\cdot}_O$ is the operator norm \cite{conway07}.
	Because of the assuption in \cref{cond:feature-condition}, $\psi$ is bounded.
	This implies that the adjoint $\psi^\T$ is, too.
	We already saw that $\alpha$ is constant.
	It remains to show the boundedness of $\dot{p_i}$.
	First consider each entry $\dot{p}_{i, j}$ of $\dot{p}_i$.
	By Cauchy-Schwarz we get
	\begin{align}
		\abs{p_{i,j}} &= \abs{\frac{\partial}{\partial q_{i, j}}\left<p_i, \psi^\T(q_i) \alpha \right> }\\
		&= \abs{\left<p_i, \frac{\partial}{\partial q_{i, j}} \psi^\T(q_i) \alpha \right> }\\
		&\leq \norm{p_i}_\cX \cdot \norm{\frac{\partial}{\partial q_{i, j}} \left(\psi^T(q_i) \alpha\right) }_\cX \ .
	\end{align}
	Observe the second factor, $\psi^\T(q_i) \alpha$. $\psi^\T$ is a matrix (actually a bounded linear operator in $L(\cF, \cX)$, but we are finite-dimensional), and $\alpha \in \cF$.
	The matrix-vector product is a continuous, bilinear operator and thus we can apply the product rule for partial derivatives.
	$\alpha$ does not depend on $q_{i, j}$, thus the equality $\frac{\partial}{\partial q_{i, j}} \left(\psi^T(q_i) \alpha\right) = \left(\frac{\partial}{\partial q_{i, j}} \psi^T(q_i)\right)  \alpha$ holds true. The partial derivative of $\psi^\T$ is again a bounded linear operator.
	\begin{equation}
		\norm{\left(\frac{\partial}{\partial q_{i, j}} \psi^T(q_i)\right)  \alpha}_\cX \leq \norm{\frac{\partial}{\partial q_{i, j}} \psi^T(q_i)}_O \norm{\alpha}_\cF \ .
	\end{equation}
	Together with the already established inequality we get
	\begin{align}
		\abs{\dot{p}_{i,j}} &\leq \norm{p_i}_\cX \cdot \norm{\frac{\partial}{\partial q_{i, j}} \left(\psi^T(q_i)\right) \alpha }_\cX\\
		&\leq  \norm{p_i}_\cX \cdot \norm{\frac{\partial}{\partial q_{i, j}} \psi^T(q_i)}_O \cdot \norm{\alpha }_\cF \ .
	\end{align}
	In total, this gives
	\begin{align}
		\norm{\dot{p}_i}^2 &= \sum_{i=1}^N \dot{p}_{i,j}^2\\
		&\leq \norm{p_i}_\cX^2 \cdot \norm{\alpha }_\cF^2 \cdot \sum_{i=1}^N  \norm{\frac{\partial}{\partial q_{i, j}} \psi^T(q_i)}_O^2\\
		& = \norm{p_i}_\cX^2 \cdot \norm{\alpha }_\cF^2 \cdot \norm{\grad \psi^T(q_i)}^2 \\
		& \leq \norm{p_i}_\cX^2 \cdot \norm{\alpha }_\cF^2 \cdot \sup_{x \in \cX}\norm{\grad \psi^T(x)}^2 \ ,
	\end{align}
	which is, of course, equivalent to 
	\begin{equation}
	\label{eq:norm-p-inequality}
		\norm{\dot{p}_i}\leq \norm{p_i}_\cX \cdot \norm{\alpha }_\cF \cdot \sup_{x \in \cX}\norm{\grad \psi^T(x)} \ .
	\end{equation}
	All factors on the right side are bounded: $p_i$ is a continuous function defined on a compact interval, $\alpha$ is constant, as we have seen, and the last term by \cref{cond:feature-condition}.
	This means $\dot{p_i}$ and therefore $\dot{p}$ are bounded, too.
\end{proof}


\subsection{Geodesic Shooting}

\subsection{Continuous Limit and Adherence Values}

\input{chapters/continuous_limit}