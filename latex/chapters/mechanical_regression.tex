\section{Mechanical Regression}
\label{sec:mechanical-regression}

In \cref{sec:neural-networks} we have seen that neural networks are a popular and extremely successful way of approximating a solution to the supervised learning problem.
ResNets in particular enable the use of very deep architectures.
In this section, we take a closer look at a simplified model of ResNets and derive their continuous limit as the number of residual blocks tends towards infinity.
On the way, we show that the ResNet regression approach can be regarded as a discrete mechanical system following Hamilton's stationary action principle -- thus the term \emph{mechanical regression}.
In the limit, this mechanical system converges towards a continuous version of the same principle, which proves to be equivalent to what in image registration is known as geodesic shooting \cite{allassonniere05}.

\citet{owhadi20} compares these results to the problem formulations used in image shape analysis, computational anatomy and image registration.
He views the continuous solution as a generalization of image registration and calls it \emph{idea registration}, arguing that just as in the former, data points are aligned through various transformations in their respective feature spaces.
The difference lies in the fact that in idea registration the data points can be arbitrary rather than mere landmarks and the spaces can be high dimensional (compared to the two or three dimensional spaces images lie in).

This section essentially follows chapter 3 of \cite{owhadi20} and aims to provide a profound and comprehensive explanation of mechanical regression and how it can be interpreted as the continuous limit of ResNets.
\cref{fig:convergence} gives an overview about the problems and theorems presented in this section and can be used as a look-up table.

\subsection{Modeling Residual Neural Networks}
\label{sec:resnet-model}

Recall the supervised learning problem: Given training data $X$ and $Y$ consisting of $X_i \in \cX$, $Y_i \in \cY$ and $f^\dagger(X) = Y$, approximate $f^\dagger$.
One possibility to solve this problem are residual neural networks, which approximate the target function $f^\dagger$ through a series of residual blocks.
In mathematical terms we can write
\begin{equation}
	f^\ast \coloneqq f \circ \Phi_L 
\end{equation}
for the approximate solution, where
\begin{equation}
	\label{eq:resnet-blocks}
	\Phi_L \coloneqq \phi_L \circ \phi_{L-1} \circ \ldots \circ \phi_1.
\end{equation} 
$\Phi_L$ is the composition of $L$ residual blocks, that is, functions $\phi_k = I + v_k$.
Here, the $v_k$ are functions mapping the input space $\cX$ onto itself and $I$ is the identity operator.
Thus, we can regard $\Phi_L$ as a large deformation of $\cX$.
The function $f: \cX \to \cY$ the deformed space to the target space $\cY$.
Note that this model is but a simplified model of ResNets.
In reality, the residual blocks often map the input space to a different space, as is the case in most convolutional networks \cite{he16}.

By default, the $v_k$ and $f$ can be arbitrary functions.
However, it poses a challenge to not only approximate the target with any series of functions, but with such that generalize well, meaning that they also perform well on data other than the training data $(X, Y)$.
Thus, it is common in machine learning \cite{goodfellow16} to apply some form of regularization.
One popular approach, which we will also use here, is to apply penalties to the parameters' norms -- which are, in this case, the norms of $v_k$ and $f$.

In order to do that we have to define appropriate normed spaces for the functions $v_k$ and $f$.
For that, we introduce two kernels and two RKHSs of functions:
$\cV \subseteq \{f: \cX \rightarrow \cX\}$ and $\cH \subseteq \{g: \cX \rightarrow \cY\}$.
By \cref{theo:kernel-for-rkhs} there is a unique association between the kernel and its RKHS.
Let $\Gamma$ be the kernel associated with $\cV$ and $K$ that of $\cH$.
We restrict the $v_k$ and $f$ to lie in the spaces $\cV$ and $\cH$, respectively.
Then we can identify $f$ and the $v_k$ as solutions to the following problem:
\begin{problem}
	\label{prob:min-v-f}
	\begin{cases}
		\text{Minimize~} & \nu \cdot \frac{L}{2} \sum_{k=1}^{L} \norm{v_k}_\cV^2
		+ \lambda \norm{f}_\cH^2 
		+ l((f \circ \Phi_L)(X), Y) \\
		\text{such that~} & v_1, \ldots, v_L \in \cV, f \in \cH \ .
	\end{cases}
\end{problem}
Here, $\nu$ and $\lambda$ are strictly positive balancing parameters and $l$ a non-negative loss measuring the similarity of the predicted outputs, that is, the image of $X$ under $f \circ \Phi_L$.
Just as intended, $v_k$ and $f$ with large norms are penalized.

Utilizing the ridge regression loss $l_R$ (\cref{eq:ridge-regression-loss}), we can rewrite the above minimization problem as
\begin{problem}
	\label{prob:discrete-resnet}
	\begin{cases}
		\text{Minimize~} & \nu \cdot \frac{L}{2} \sum_{k=1}^{L} \norm{v_k}_\cV^2
		+ l_R(\Phi_L(X), Y) \\
		\text{such that~} & v_1, \ldots, v_L \in \cV\ .
	\end{cases}
\end{problem}
By hiding the regularization of $f$ in the loss we can, for now, focus exclusively on the functions $v_k$.
For our calculations we will only assume that $l$ is a non-negative, continuous loss function.
If desired, we can later balance the norms of the $v_k$ with the regularity of $f$ by choosing the loss appropriately.

In the calculations, we will work under the following conditions.
\begin{condition}
	\label{cond:feature-condition}\mbox{}
	\vspace*{-\parsep}
	\vspace*{-\baselineskip}
	\begin{enumerate}
		\item The function $(x_1, x_2) \rightarrow \Gamma(x_1, x_2)$ and its first and second order partial derivatives are continuous and uniformly bounded.
		\item There exists an $r > 0$ such that $\forall Z \in \cX^N:~Z^\T \bGamma(X, X) Z \geq r Z^\T Z$.
		\item $(\cX, \left< \cdot, \cdot \right >_\cX)$ and $(\cY, \left< \cdot, \cdot\right >_\cY)$ are finite-dimensional Hilbert spaces.
		\item $l: \cY^N \times \cY^N \rightarrow [0, \infty)$ is a positive and continuous loss.
	\end{enumerate}
\end{condition}
Just like in \cref{sec:optimal-recovery}, $\bGamma(X, X)$ is the block operator matrix with entries $\Gamma(X_i, X_j)$.
From \cref{cond:feature-condition} (2), we get the following implication.
\begin{lemma}
	If there exists $r > 0$ such that $\forall Z \in \cX^N:~Z^\T \bGamma(X, X) Z \geq r Z^\T Z$, the block operator matrix $\bGamma(X, X)$ is non-singular.
\end{lemma}
\begin{proof}
	If $\bGamma(X, X)$ was singular, there would exist a vector $Z \in \cX^N$ with $\norm{Z}_{\cX^N} > 0$ and $\bGamma(X, X) Z = 0$, implying $Z^\T \bGamma(X, X)Z = 0$. 
	This is contradiction.
%	Conversely, if there exists a $0 \neq Z \in \cX^N$ such that $Z^\T \bGamma(X, X) Z < \epsilon Z^\T Z$ for all $\epsilon > 0$, then $\bGamma(X, X) Z = 0$.
\end{proof}
Ideally, we would not need additional conditions like those introduced above.
They are, however, automatically satisfied in most real world examples and ease calculations and proofs considerably.

\subsection{Discrete Stationary Action Principle}

\input{chapters/discrete_least_action_principle}

\subsection{Continuous Stationary Action Principle}

\input{chapters/least_action_principle}

\subsection{Hamiltonian Representation}

\input{chapters/hamiltonian_representation}

\subsection{Geodesic Shooting}

Using the Hamiltonian formalism, we can now formulate another problem equivalent to \cref{prob:cont-least-action} in which the search for the optimal trajectory $q$ is reduced to the search for the optimal initial momentum $p(0)$.
A similar method, called \emph{geodesic shooting} has been introduced by \citet{allassonniere05} in the field of image registration.
In this method, an optimal control problem is also reduced to finding optimal initial momenta for a Hamiltonian system.
For mechanical regression, the equivalent problem is given by
\begin{problem}
	\label{prob:geodesic-shooting}
	\begin{cases}
		\text{Minimize~}& \frac{\nu}{2} p(0)^\T \bGamma(X, X)p(0) + l(q(1), Y)\\
		\text{such that~} & p(0) \in \cX^N,\ q(0) = X,\ p = \bGamma(q, q)^{-1}\dot{q}\\
		&\text{and~} (q,p) \text{~follow Hamilton's equations \ref{eq:hamiltonian-system}} \ .
	\end{cases}
\end{problem}
For later use, we explicitly define the objective function of this problem:
\begin{equation}
\label{eq:geodesic-shooting-objective}
\fV(p(0), X, Y) \coloneqq \frac{\nu}{2} p(0)^\T \bGamma(X, X)p(0) + l(q(1), Y) \ .
\end{equation}

The following theorem states the equivalence between \cref{prob:cont-least-action} and \cref{prob:geodesic-shooting}, which means that the least action formulation can be solved by geodesic shooting.
\begin{theorem}
	\label{theo:geodesic-shooting}
	$q \in C^1([0, 1], \cX^N)$ minimizes \cref{prob:cont-least-action} if and only if $p = \bGamma(q, q)^{-1}\dot{q}$ and $p(0)$ minimizes \cref{prob:geodesic-shooting}.
\end{theorem}
\begin{proof}
	Let $q$ minimize \cref{prob:cont-least-action} and define $p = \bGamma(q, q)^{-1}\dot{q}$.
	Then $q(0) = X$ and by \cref{theo:hamiltonian-dynamic} $(q, p)$ satisfy Hamilton's equations \ref{eq:hamiltonian-system}.
	We have $\fL(t, q(t), \dot{q}(t)) = \fH(t, q(t), p(t))$ and know from \cref{cor:energy-preservation} that $\fH$ is constant across time, that is $\fH(t_1, q(t_1), p(t_1)) = \fH(t_2, q(t_2), p(t_2))$ holds for all $t_1, t_2 \in [0, 1]$ .
	This implies that
	\begin{equation}
	\mathcal{A}(q) = \int_{0}^{1} \fL(t, q(t), \dot{q}(t)) \mathrm{d}t 
	= \int_{0}^{1} \fH(t, q(t), p(t)) \mathrm{d}t = \fH(t_1, q, p) \ .
	\end{equation}
	Therefore $(q, p)$ minimize $\nu \fH(t_1, q(t_1), p(t_1) + l(q(1), Y)$ and for $t_1 = 0$ we get
	\begin{align}
	\nu \fH(0, q(0), p(0)) + l(q(1), Y) &= \frac{\nu}{2} p(0)^\T \bGamma(q(0), q(0))p(0) + l(q(1), Y)\\
	&= \frac{\nu}{2} p(0)^\T \bGamma(X, X)p(0) + l(q(1), Y) \ ,
	\end{align}
	which is the target function $\fV$ of \cref{prob:geodesic-shooting}.
	Vice versa, if $p(0)$ minimizes $\fV$ and $(q, p)$ follow Hamilton's equations, $q$ minimizes \cref{prob:cont-least-action} using the same argument.
\end{proof}

\subsection{Continuous Limit and Adherence Values}

\input{chapters/continuous_limit}