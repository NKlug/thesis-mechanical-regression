Take a closer look at the first term of the objective function in \cref{prob:min-q} (the parameter $\nu$ has been deliberately omitted here):
\begin{equation}
	\label{eq:discrete-lagrangian}
	\frac{1}{2} \sum_{k=1}^{L} \left(\frac{q_{k+1} - q_k}{\Delta t}\right)^\mathrm{T} \mathbf{\Gamma}(q_k, q_k)^{-1} \frac{q_{k+1} - q_k}{\Delta t} \Delta t \ .
\end{equation}
With some imagination this very much looks like an approximation of the integral of some continuous function with a step function using $L$ intervals of width $\Delta t$.
This would require the sequence $q_k$ to be a discrete equidistant sampling of a function $q: A \rightarrow \cX^N$, where $A$ is an interval in $\R$.
Without loss of generality we choose $A = [0, 1]$ and write $q_k = q(k \Delta t)$.
\Todo{Nice wording: The dot signifying the time derivative d dt	}
This way, $\frac{q_{k+1} - q_k}{\Delta t}$ can be interpreted as a forward differential quotient approximating the derivative $\dot{q}$ on at the point $k \Delta t = \frac{k}{L}$ and we get the approximation
\begin{equation}
	\left(\frac{q_{k+1} - q_k}{\Delta t}\right)^\mathrm{T} \mathbf{\Gamma}(q_k, q_k)^{-1} \frac{q_{k+1} - q_k}{\Delta t} 
	\approx \dot{q}\left(\frac{k}{L}\right)^\mathrm{T} \mathbf{\Gamma}\left(q\left(\frac{k}{L}\right), q\left(\frac{k}{L}\right)\right)^{-1}\dot{q}\left(\frac{k}{L}\right)
\end{equation}
This leads us to the obvious question if the solutions of the problem in \cref{prob:min-q} are also approximating the solutions of a similar continuous problem.
As it turns out, they do.
Consider 
\begin{equation}
\label{eq:action}
	\mathcal{A}(q) \coloneqq \int_{0}^{1} \fL(q, \dot{q}, t) \mathrm{d}t \ ,
\end{equation}
with

\begin{equation}
\label{eq:lagrangian}
	\fL(q, \dot{q}, t) \coloneqq \frac{1}{2} \dot{q}(t)^\mathrm{T} \mathbf{\Gamma}(q(t), q(t))^{-1} \dot{q}(t) \ .
\end{equation}
\Todo{Mention difference and similarity between physical and mathematical approach in definition of Lagrangian}
Here, the Lagrangian is a quadratic form.
In a physical interpretation, the Lagrangian above resembles that of a system of free moving particles who don't interact with each other.
It usually represents the kinetic energy $T$ of the system.
This interpretation would imply that in our case, the potential energy $U$ of the system is $0$.

As a result, we get the following theorem.
\begin{theorem}
	The minimal value of \cref{prob:min-q} converges towards the minimal value of             
	\info{Theorem 3.11 from paper} 
	\begin{problem}
	\label{prob:cont-least-action}
		\begin{cases}
			\text{Minimize~} & \nu \mathcal{A}(q) + l(q(1), Y)\\
			\text{such that~} & q \in C^1([0,1], \cX^N),\ q(0) = X \ .
		\end{cases}
	\end{problem}
\end{theorem}
As usual, $C^1([0,1], \cX^N)$ is the set of continuously differentiable functions $q: [0, 1] \rightarrow \cX^N$.
We will save the proof for later.

\Todo{Link between mathematical approach, i.e. variational calculus, and physical approach: Physical "interpretation" can be derived from mathematical approach + some axioms}
Equations \ref{eq:action} and \ref{eq:lagrangian} closely resemble characteristic equations from the framework of Lagrangian and Hamiltonian mechanics.
Within that framework, $q$ is the trajectory of the particle, $\fL$ the \emph{Lagrangian function} and $\mathcal{A}$ the \emph{Action}.
In our case, each training sample constitutes a particle, therefore $q$ -- a vector -- describes the trajectories of not one, but $N$ particles.
We will now explore if the rich theory of Lagrangian physics can help us analyze our problem.

The action is the integral over the Lagrangian between two points in time $t_1, t_2$.
Here, we chose $t_1 = 0$ and $t_2 = 1$.
One core principle in Lagrangian Mechanics is \emph{Hamilton's Principle}, also called the \emph{Principle of Least Action}.
It states that
\Todo{Citation needed!}
the evolution of $q$ over time between two states $q(t_1)$ and $q(t_2)$ at times $t_1$ and $t_2$ is a stationary point of the action $\mathcal{A}$.
We do not have the time to dive deeper into the underlying theory, but the interested reader is referred to \Todo{References, one of them could be Feynman lectures 2 chapter 19}.
Note that "Principle of Least Action" is a misnomer and should rather be "Principle of Stationary Action".


Hamilton's principle has a strong physical background:
The true evolution of a system $q$ between two states $q(t_1)$ and $q(t_2)$ is a stationary point of the action.
\Todo{Why doesn't the endpoint have to be fixed? This is e.g. mentioned in Classical Mechanics by Goldstein p45}

The path of least action need not always be the path for which the action becomes minimal, but rather stationary.
However, if we can show that there exists a minimum of \cref{prob:cont-least-action}, this will, \Todo{Why exactly -- do we have some similar theorem like "partial derivative zero implies extremum?"}by ..., be a point of stationary action

Let $q(1)$ be arbitrary but fixed.
Then a minimizer $q^\ast$ of \cref{prob:cont-least-action} with $q^\ast(1) = q(1)$ is a minimizer of $\mathcal{A}(q)$.
By [Kielh√∂fer - Variationsrechnung, Satz 1.4.2] it follows that $q^\ast$ fulfills the Euler-Lagrange equation:
\begin{equation}
	\frac{\mathrm{d}}{\mathrm{d}t} \frac{\partial \fL}{\partial \dot{q}} - \frac{\partial \fL}{\partial q} = 0 \ .
\end{equation}
Bear in mind that $q$ is a vector.
So to be precise, we would have to write $N$ Euler-Lagrange equations, one for each $q_i$ (just imagine $q_i$ and $\dot{q}_i$ in the equation above).
Nonetheless, in the following we can always treat all $q_i$ the same.
So for the sake of readability and convenience we will stick to the vector notation, which together with block operator matrices allows concise formulation.
The Euler-Lagrange equation with the Lagrangian from \cref{eq:lagrangian} reads
\begin{equation}
\label{eq:concrete-lagrangian}
	\frac{\mathrm{d}}{\mathrm{d}t} \left(\mathbf{\Gamma}(q, q)^{-1} \dot{q} \right)
	= \frac{\partial}{\partial q} \left(\frac{1}{2} \dot{q}^\mathrm{T} \mathbf{\Gamma}(q, q)^{-1} \dot{q}\right) \ .
\end{equation}

As this is true for all endpoints $q(1)$, it especially holds true for the value for which the whole expression is minimal.