\section{Introduction}

Ideas on what to write here:
\begin{itemize}
	\item Neural Networks are universal approximators
	\item Why neural networks work so well \cite{lin17}
	\item It is not entirely clear why neural networks work so well. 
	They were inspired by the neurological structure of the human brain.
	No real mathematical explanation.
\end{itemize}

Argument:
Deep models are better than flat ones. An explanation for this might be \cite{lin17}.
However, before ResNets, models showed deteriorating performance from certain depths onward.
Here: Conjecture that NNs have a hard time learning the identity.
\cite{he16} proposed a clever trick to prevent deteriorating performance:
Residual Connections.

Briefly explain ResNets and give results of experiments with ResNets.

Somehow connect this with the paper and briefly sum up what happens in the paper.

The title of the paper is a very philosophical question.
In this thesis, however, we will focus rather on the mathematical side and examine \cite{owhadi20} up to but excluding chapter 4.

Introduction to main problem at hand:
Approximating $f^\dagger$ with the known mapping $f^\dagger(X) = Y$ with a function $f^\ddagger$.
This is essentially regression, however can also be classification (explanation and references needed!).

Notation: $X$ is set/ array, $X_i$ is $i$-th component

\subsection{Overview}
This thesis is structured in the following way: