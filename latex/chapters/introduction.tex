\section{Introduction}

Ideas on what to write here:
\begin{itemize}
	\item Neural Networks are universal approximators
	\item Why neural networks work so well \cite{lin17}
	\item It is not entirely clear why neural networks work so well. 
	They were inspired by the neurological structure of the human brain.
	No real mathematical explanation.
\end{itemize}
Structure
\begin{enumerate}
	\item some lame first sentences :/
	\item introduction to supervised learning
	\item we can solve supervised learning very well with neural networks
	\item give some resource for basics of neural networks - we ain't got time to explain
	\item transition to the paper: what does the author explore?
	\item outline
\end{enumerate}

Argument:
Deep models are better than flat ones. An explanation for this might be \cite{lin17}.
However, before ResNets, models showed deteriorating performance from certain depths onward.
Here: Conjecture that NNs have a hard time learning the identity.
\cite{he16} proposed a clever trick to prevent deteriorating performance:
Residual Connections.

Briefly explain ResNets and give results of experiments with ResNets.

Somehow connect this with the paper and briefly sum up what happens in the paper.

The title of the paper is a very philosophical question.
In this thesis, however, we will focus rather on the mathematical side and examine \cite{owhadi20} up to but excluding chapter 4.

Introduction to main problem at hand:
Approximating $f^\dagger$ with the known mapping $f^\dagger(X) = Y$ with a function $f^\ddagger$.
This is essentially regression, however can also be classification (explanation and references needed!).

Notation: $X$ is set/ array, $X_i$ is $i$-th component

A part of the success of neural networks can be explained not only by mathematics but also physics.
It seems to be the case that neural networks are not very cost efficient in approximating any function, but are for most functions humans are interested in.



\subsection{Overview}
This thesis is structured in the following way:

\subsection{Notation}
We'll have to see if this section is required.
\begin{itemize}
	\item $[n] \coloneqq \{1, 2, \ldots, n\}$ for the standard $n$-set for $n \in \mathbb{N}$
	\item $X, Y$ are given datapoints
	\item We seek to approximate a function $f^\dagger$.
\end{itemize}