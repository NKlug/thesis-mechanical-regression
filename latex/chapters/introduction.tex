\section{Introduction}

Machine learning involving neural networks has been one of the fastest growing field of research that ever existed (ref!).
Their success in ... is unpreceded.
However, the driving force in machine learning, neural networks, are still not fully understood from a theoretical point of view.
They are like an "elephant in a dark room", and showing properties from various mathematical fields.
In an effort to shed light on their nature, in "Do Ideas Have Shape Plato's Theory of Forms as the Continuous Limit of Artificial Neural Networks" \cite{owhadi20}, the author analyzes neural networks by modeling a special class of neural networks which are called residual neural networks (ResNets).
In this thesis, parts of this paper are presented in depth.

Ideas on what to write here:
\begin{itemize}
	\item Neural Networks are universal approximators
	\item Why neural networks work so well \cite{lin17}
	\item It is not entirely clear why neural networks work so well. 
	They were inspired by the neurological structure of the human brain.
	No real mathematical explanation.
	\item A short note that the thesis assumes basic knowledge of neural networks.
\end{itemize}
Structure
\begin{enumerate}
	\item some lame first sentences :/
	\item introduction to supervised learning
	\item we can solve supervised learning very well with neural networks
	\item give some resource for basics of neural networks - we ain't got time to explain
	\item transition to the paper: what does the author explore?
	\item outline
\end{enumerate}

Argument:
Deep models are better than flat ones. An explanation for this might be \cite{lin17}.
However, before ResNets, models showed deteriorating performance from certain depths onward.
Here: Conjecture that NNs have a hard time learning the identity.
\cite{he16} proposed a clever trick to prevent deteriorating performance:
Residual Connections.

Briefly explain ResNets and give results of experiments with ResNets.

Somehow connect this with the paper and briefly sum up what happens in the paper.

The title of the paper is a very philosophical question.
In this thesis, however, we will focus rather on the mathematical side and examine \cite{owhadi20} up to but excluding chapter 4.

Introduction to main problem at hand:
Approximating $f^\dagger$ with the known injective mapping $f^\dagger(X) = Y$ with a function $f^\ddagger$.
This is essentially regression, however can also be classification (explanation and references needed!).
Wlog We only regard the case where the $X_i$ are pairwise distinct.
Otherwise the pair $(X_i, Y_i)$ would occur twice.

Notation: $X$ is set/ array, $X_i$ is $i$-th component

A part of the success of neural networks can be explained not only by mathematics but also physics.
It seems to be the case that neural networks are not very cost efficient in approximating any function, but are for most functions humans are interested in.

The goal of this thesis is to present chapter 3 of owhadi and provide/serve as a foundation to the further understanding of the rest of the paper...

\subsection{Overview}
This thesis is structured in the following way:

\subsection{Notation}
We'll have to see if this section is required.
\begin{itemize}
	\item $[n] \coloneqq \{1, 2, \ldots, n\}$ for the standard $n$-set for $n \in \mathbb{N}$
	\item $X, Y$ are given datapoints
	\item We seek to approximate a function $f^\dagger$.
\end{itemize}