\input{figures/figure_convergence_sketch}

The only thing left to prove now is that as $L \rightarrow \infty$, \cref{prob:min-q} does indeed converge towards \cref{prob:cont-least-action}.
We conduct this proof by using the equivalence of \cref{prob:min-q} to a discrete geodesic shooting problem, which we will briefly introduce.
For completeness, we will furthermore show a continuous equivalent of the ResNet formulation in \cref{prob:min-v-f}.
With these preparations, we can then continue to prove that in fact, all three discrete problems converge towards their continuous counterparts.
A depiction of all problems and their equivalences can be seen in \cref{fig:convergence}.

\subsubsection{Discrete Geodesic Shooting}

We will briefly introduce the discrete geodesic shooting formulation.
Its derivation involves the theory of discrete Lagrangian and Hamiltonian mechanics (see for example \cite{west04}) and is similar to the continuous case.
Within this thesis, we can only review the results.

Consider the discrete Hamiltonian system
\begin{equation}
	\label{eq:discrete-hamiltonian-system}
	\begin{split}
		q_{k+1} &= q_k + \Delta t \bGamma(q_k, q_k) p_k\\
		p_{k+1} &= p_k + \frac{\Delta t}{2} \grad_{q_{k+1}} \left(p_{k+1}^\T \bGamma(q_{k+1}, q_{k+1}) p_{k+1}\right)\ .
	\end{split}
\end{equation}
The discrete geodesic shooting formulation is as follows:
\begin{problem}
	\label{prob:discrete-geodesic-shooting}
	\begin{cases}
		\text{Minimize~} & \frac{\nu}{2} \sum_{k=1}^L p_k^\T \bGamma(q_k, q_k) p_k \Delta t + l(q_{L+1}, Y)\\
		\text{such that~} & p_1 \in \cX^N,\ q_1 = X,\ p_k = \bGamma(q_k, q_k)^{-1} \frac{q_{k+1} - q_k}{\Delta t},\ \Delta t = \frac{1}{L} \\
		&\text{~and~} (q_k, p_k) \text{~follow the discrete Hamiltonian equations \ref{eq:discrete-hamiltonian-system}}\ .
	\end{cases}
\end{problem}
Let $\fV_L(p_1, X, Y)$ denote the problem's objective function.
We get the following result.
\begin{theorem}
	\label{theo:discrete-shooting-min-q-equivalence}
	$q_1, \dots, q_{L+1}$ minimizes \cref{prob:min-q} if and only if $p_k \coloneqq \bGamma(q_k, q_k)^{-1} \frac{q_{k+1} - q_k}{\Delta t}$ and $p_1$ minimizes \cref{prob:discrete-geodesic-shooting}.
\end{theorem}

\subsubsection{Continuous Residual Neural Network} 
%This is equation 1.12 from \cite{owhadi20}.
%See how this fits here
%\begin{equation}
%	\label{eq:phi-v-differential-equation}
%	\dot{\Phi}^v(x, t) = \mathbf{\Gamma}(\Phi^v(x, t), q) p
%\end{equation}
In this section we will derive a continuous formulation of the ResNet problem in \ref{prob:min-v-f} and show that it is equivalent to the continuous least action formulation \ref{prob:cont-least-action}.

Let $C([0, 1], \cV)$ be the space of continuous functions $v: [0, 1] \rightarrow \cV$, such that the function $v(t)(x)$ is globally Lipschitz in $t$ and $x$.
For simplicity, we will write $v(t, x)$ instead of $v(t)(x)$.
Let $\Phi_v \in C([0, 1], \cV)$ be the solution to the following initial value problem, whose existence is implied by the Picard-LindelÃ¶f theorem \cite[Theorem 1.2.3]{arino06}.
\begin{equation}
	\label{eq:phi-v-differential-equation}
	\begin{cases}
		&\dot{\Phi}(t, x) = v(\Phi(t, x))\\
		&\Phi(0, x) = x \ .
	\end{cases}
\end{equation}
$\dot{\Phi}(t, x)$ denotes the time derivative $\frac{\mathrm{d}}{\mathrm{d}t}\Phi$.
The idea behind this differential equation is that it is the continuation of $\Phi_L$ \ref{eq:resnet-blocks} as $L \rightarrow \infty$  in the sense that for $k \in [L]$, $\phi_k \circ \phi_{k-1} \circ \cdots \circ \phi_1$ is an approximation to $\Phi_v(\frac{k}{L}, \cdot)$.
Similarly, the $v_k$ from the discrete ResNet formulation approximate $v(\frac{k}{L}, \cdot)$.

We can now formulate the continuous ResNet problem.
\begin{problem}
	\label{prob:resnet-limit}
	\begin{cases}
		\text{Minimize~}& \frac{\nu}{2} \int_{0}^{1} \norm{v(t)}_\cV^2~\mathrm{d}t
		+ l(\Phi_v(1, X), Y)\\
		\text{such that~}& v \in C([0, 1], \cV)\\
	\end{cases}
\end{problem}
Here, $\Phi_v(1, X)$ denotes the vector $(\Phi_v(t, X_i))_{i \in [N]}$.

The following theorem is the continuous analogue to \cref{theo:v-q-problem-equivalence}.
\begin{theorem}
	\label{theo:v-q-continuous-problem-equivalence}
	$v$ minimizes \cref{prob:resnet-limit} if and only if $v$ fulfills
	\begin{equation}
			\label{eq:v-q-differential-equation}
			\dot{\Phi}_v(t, x) = \mathbf{\Gamma}(\Phi_v(x, t), q(t)) \bGamma(q(t), q(t))^{-1} \dot{q}(t)
	\end{equation}
	together with the initial condition that $\Phi_v(0, x) = x$ for all $x \in X$ and $q$ minimizes \cref{prob:cont-least-action}.
\end{theorem}
The proof of this theorem is very similar to that of \cref{theo:v-q-problem-equivalence}, with the difference that the discrete elements $q_k \in \cX^N$ are replaced by one continuous function $q: [0, 1] \rightarrow \cX^N$.
Within the scope of this thesis, we can only review the main ideas.
\begin{proof}
	Let $v$ be a minimizer of \cref{prob:resnet-limit}.
	First, we define the function $q: [0, 1] \rightarrow \cX^N$ as the vector with elements
	\begin{equation}
		q_i(t) \coloneqq \Phi_v(t, X_i) \ .
	\end{equation}
	In vector notation we get $\Phi_v(1, X) = q(1)$.
	As $\Phi_v$ follows the differential equation in \ref{eq:phi-v-differential-equation}, it follows that 
	\begin{equation}
		\dot{q}(t) = \dot{\Phi}_v(t, X) = v(t, \Phi_v(t, X)) = v(t, q(t)) \ .
	\end{equation}
	\info{v must minimize the integral at each point}
	Now, for fixed $t \in [0, 1]$, we face an optimal recovery problem:
	\begin{equation}
		\min\{\norm{v(t, \cdot)}_\cV^2\ |\ v(t, q(t)) = \dot{q}(t)\} \ .
	\end{equation}
	\cref{eq:optimal-recovery-f} provides an explicit solution to this problem, which is:
	\begin{equation}
		v(t, \cdot) = \bGamma(\cdot, q(t))\bGamma(q(t), q(t))^{-1}\dot{q}(t) \ .
	\end{equation}
	Evaluating this function at $\Phi_v(t, X)$ yields \cref{eq:v-q-differential-equation}.
	From \cref{eq:optimal-recovery-loss-closed-form} it follows that 
	\begin{equation}
		\norm{v(t, \cdot)}_\cV^2 = \dot{q}(t) \bGamma(q(t), q(t))^{-1} \dot{q}(t) = 2 \fL(t, q(t), \dot{q}(t))  \ .
	\end{equation}
	This and $\Phi_v(1, X) = q(1)$ implies that $q$ minimizes \cref{prob:cont-least-action}.
\end{proof}

\subsubsection{Existence of Minimizers}

We already saw the equivalence of \cref{prob:cont-least-action,prob:geodesic-shooting,prob:resnet-limit}, in the sense that their minimizers bijectively correspond to each other -- that is, if they exist.
Now we will show that there do indeed exist such minimizers and also that the problems' minimal values are identical.
\begin{theorem}[Existence of minimizers for the continuous problems]
	\label{theo:continuous-solutions-existence}
		There exist minimal points for \cref{prob:cont-least-action,prob:geodesic-shooting,prob:resnet-limit} and their minimal objective values are identical.
\end{theorem}

\begin{proof}
	First, we show the existence of minimizers.
	\cref{theo:geodesic-shooting,theo:v-q-continuous-problem-equivalence} state that we can obtain a minimizer for each problem from a minimizer $p(0)$ of the geodesic shooting formulation (\cref{prob:geodesic-shooting}).
	Hence we just have to show the existence of such a $p(0)$.
	Define the ball $B_\rho \coloneqq \{p(0) \in \cX^N\ | \ \norm{p(0)}_{\cX^N}^2 \leq \rho^2 \}$. Here, $\norm{\cdot}_{\cX^N}$ is the norm induced by the inner product on the product space $\cX^N$.
	Since we required $\cX$ to be finite dimensional, so is $\cX^N$.
	This implies that $B_\rho$ is compact.
	We now want to show the existence of a local minimum of the function $p(0) \mapsto \fV(p(0), X, Y)$ on $B_\rho$.
	If we can show that $\fV$ is continuous in $p(0)$, this follows from the extreme value theorem.
	
	The first term of $\fV$ is a quadratic form in $p(0)$ and hence continuous.
	The second term, the loss $l$, is continuous in $q(1)$.
	In order to see that $q(1)$ is continuous in $p(0)$ we use a result from the theory of ODEs.
	Again, write
	\begin{equation}
	\left(\dot{q}(t), \dot{p}(t)\right)^\T = f\left(t, \left(q(t), p(t)\right)^\T \right)
	\end{equation}
	for the Hamiltonian system in \cref{eq:hamiltonian-system}.
	We already know that $f$ is bounded for $t \in [0, 1]$, because $q(t)$ and $p(t)$ are.
	As $f$ is continuous in $\left(q(t), p(t)\right)^\T$ and so is its Jacobian, as one can easily verify.
	Furthermore, we know from the proof of \cref{theo:hamiltonian-system-solution} that the Jacobian is bounded.
	Let $(q_i, p_i)^\T$ be the unique solution to the Hamiltonian system $f$ with initial conditions $q_i(0) = X, p(0) = p_{0, i}$ for $i \in [2]$, which exists according to \cref{theo:hamiltonian-system-solution}.
	Now, we use a result from the theory of ODEs:
	From \cite[Theorem~1.4.1]{arino06} it follows as a special case that for all $\epsilon > 0$ there exists $\delta > 0$ such that
	\begin{equation}
	\norm{p_{0, 1} - p_{0, 2}}_{\cX^{2N}} < \delta \Rightarrow 
	\left(\forall t \in [0, 1]:\ \norm{(q_1(t), p_1(t))^\T - (q_2(t), p_2(t))^\T}_{\cX^{2N}} < \epsilon \right) \ .
	\end{equation}
	Since $\norm{\cdot}_{\cX^{2N}}$ is the norm that is induced on the product space, it follows that especially $\norm{q_1(t) - q_2(t)}_{\cX^N} < \epsilon$, which proves the continuity of $q(1)$ in $p(0)$.
	This shows that $B_\rho$ contains a local minimum.
	
	\cref{cond:feature-condition} (2) states that there exists an $r > 0$ such that for all $Z \in \cX^N$ $Z^\T \bGamma(X, X) Z \geq r Z^\T Z = r \norm{Z}_{\cX^N}^2$.
	Moreover, \cref{cond:feature-condition} (4) requires the loss $l$ to be non-negative.
	These two facts imply that
	\begin{align}
		\lim_{\norm{p(0)} \rightarrow \infty} \fV(p(0), X, Y) 
		&= \lim_{\norm{p(0)} \rightarrow \infty} p(0)^\T \bGamma(X, X) p(0) + l(q(1), Y)\\
		& \geq 	\lim_{\norm{p(0)} \rightarrow \infty} p(0)^\T \bGamma(X, X) p(0) \\
		& \geq \lim_{\norm{p(0)} \rightarrow \infty} r \norm{p(0)}_{\cX^N}^2
		= \infty \ .
	\end{align}
	This means that there exists a $\rho > 0$ such that $B_\rho$ contains a global minimizer of $\fV$ and even a $\rho > 0$ such that all global minimizers are contained in $B_\rho$.
	
	The identity of the minimal values can be deduced directly from the \cref{theo:v-q-continuous-problem-equivalence,theo:geodesic-shooting} and their proofs.
\end{proof}

An equivalent result can be derived for the discrete problems.
\begin{theorem}[Existence of minimizers for the discrete problems]
	\label{theo:discrete-solutions-existence}
	There exist minimal points for \cref{prob:discrete-geodesic-shooting,prob:min-v-f,prob:min-q} and their minimal values are identical.
\end{theorem}
The proof is practically identical to the proof of the continuous case and will not be conducted here.

\subsubsection{Convergence}

With all preparations completed, we can now prove the convergence of the discrete problems towards their continuous counterparts.
For that, let $M_L(X, Y)$ be the set of minimizers of \cref{prob:discrete-geodesic-shooting}.
Respectively, let $M(X, Y)$ be the set of minimizers of \cref{prob:geodesic-shooting}.
\cref{theo:continuous-solutions-existence,theo:discrete-solutions-existence} imply that both sets are non-empty.
\begin{theorem}
	\label{theo:problem-convergence}
	The minimal value of \cref{prob:discrete-geodesic-shooting,prob:min-q,prob:min-v-f} converges towards the minimal value of \cref{prob:cont-least-action,prob:geodesic-shooting,prob:resnet-limit} as $L \rightarrow \infty$.
	Furthermore, the adherence values of $M_L(X, Y)$ are contained in $M(X, Y)$:
	\begin{equation}
	\label{eq:limit-adherence}
		\bigcap_{L' \in \mathbb{N}} \closure\left(\bigcup_{L' \geq L} M_L(X, Y)\right) \subseteq M(X, Y) \ .
	\end{equation}
\end{theorem}
Here, $\closure (A)$ is the closure of the set $A$.
We will sketch the proof:
\begin{proof}
	From the previous equivalence theorems we know that we can parameterize the minimal values and points by the initial momenta $p_1$ and $p(0)$ for the discrete \ref{prob:discrete-geodesic-shooting} and continuous geodesic shooting problem \ref{prob:geodesic-shooting} respectively.
	This means it suffices to show the convergence of $\fV_L(\cdot, X, Y)$ towards $\fV(\cdot, X, Y)$.
	Let $p_1$ minimize $\fV_L$.
	Then $p_1, \ldots, p_L$, $q_1, \ldots, q_L$ follow the discrete Hamiltonian system \ref{eq:discrete-hamiltonian-system}.
	It can be shown that this system converges uniformly to the continuous Hamiltonian system in \cref{eq:hamiltonian-system} in the sense that as $L \rightarrow \infty$, the interpolation of the solutions $p_k, q_k, k \in [L]$ converges towards a trajectory that follows the continuous Hamiltonian system.
	This implies
	\begin{equation}
		\frac{\nu}{2} \sum_{k=1}^L p_k^\T \bGamma(q_k, q_k) p_k \Delta t + l(q_{L+1}, Y) 
		\stackrel{L \rightarrow \infty}{\longrightarrow} \frac{\nu}{2} \int_{0}^{1} p(t)^\T \bGamma(q(t), q(t)) p(t) \mathrm{d} t + l(q(1), Y) \ . 
	\end{equation}
	But because the Hamiltonian is constant along its trajectory, the latter term is equal to $\frac{\nu}{2} p(0)^\T \bGamma(X, X) p(0) + l(q(1), Y)$.
	This shows the uniform convergence of $\fV_L(\cdot, X, Y)$ towards $\fV(\cdot, X, Y)$.
	
	In the proof of \cref{theo:continuous-solutions-existence} we saw that there exists a $\rho$ such that $B_\rho$ contains all global minima of $\fV$.
	This can also be shown for $\fV_L$, where $\rho$ can be chosen independently of $L$.
	Hence we choose $\rho$ such that all global minima of $\fV_L$ and $\fV$ are contained in $B_\rho$.
	Then it follows that
	\begin{equation}
		\lim_{L \rightarrow \infty} \min_{p_1 \in B_\rho} \fV_L(p_1, X, Y) = \min_{p_1 \in B_\rho} \lim_{L \rightarrow \infty}  \fV_L(p_1, X, Y) = \min_{p_1 \in B_\rho} \fV(p_1, X, Y) \ .
	\end{equation}
	This shows the convergence of minimal values.
	
	For the second part, let $(p_k)_{k \in \mathbb{N}},\ p_k \in M_k(X, Y)$ be a sequence of optimal initial momenta.
	Let $p(0)$ be one of its adherence values, that is, a subsequence $(p_{k_n})_{n \in \mathbb{N}}$ converges towards $p(0)$.
	Then, $\lim_{n\rightarrow \infty} \fV_{k_n}(p_{k_n}) = \fV(p(0))$ and $p(0)$ minimizes $\fV$.
	In order to see that, let $\epsilon > 0$. 
	Because of the uniform convergence of $\fV_k$ towards $\fV$, there exists an $n_0 \in \mathbb{N}$ such that for all $n \geq n_0$:
	\begin{align}
		\abs{\fV_{k_n}(p_{k_n}) - \fV(p(0))} 
		&\leq \abs{\fV_{k_n}(p_{k_n}) - \fV_{k_n}(p(0))} + \abs{\fV_{k_n}(p(0)) - \fV(p(0))}\\
		&< \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon \ .
	\end{align}
	The fact that $p(0)$ minimizes $\fV$ follows from the uniform convergence of $\fV_L$ towards $\fV$.
\end{proof}

\citet{owhadi20} claims that in \cref{eq:limit-adherence}, equality holds.
However, this could not be shown here.

This convergence result concludes the section on mechanical regression.