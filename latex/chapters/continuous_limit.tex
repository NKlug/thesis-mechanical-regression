
The only thing left to prove now is that as $L \rightarrow \infty$, \cref{prob:min-q} does indeed converge towards \cref{prob:cont-least-action}.

\input{figures/figure_convergence_sketch}

\subsection{Continuous Residual Neural Network} 
This is equation 1.12 from \cite{owhadi20}.
See how this fits here
\begin{equation}
	\label{eq:phi-v-differential-equation}
	\dot{\Phi}^v(x, t) = \mathbf{\Gamma}(\Phi^v(x, t), q) p
\end{equation}

Let $C([0, 1], \cV)$ be the space of continuous functions $v: [0, 1] \times \cV$, such that the function $v(t)(x)$ is globally Lipschitz in $t$ and $x$.
For simplicity, we will write $v(t, x)$ instead of $v(t)(x)$.
Let $\Phi_v \in C([0, 1], \cV)$ be the solution to the following initial value problem.
\begin{equation}
	\begin{cases}
		&\dot{\Phi}(t, x) = v(\Phi(t, x))\\
		&\Phi(0, x) = x \ .
	\end{cases}
\end{equation}
$\dot{\Phi}(t, x)$ signifies the time derivative $\frac{\mathrm{d}}{\mathrm{d}t}\Phi$.

The idea is that $\Phi_L \stackrel{L \rightarrow \infty}{\longrightarrow} \Phi$ in the sense that for $k \in [L]$, $\phi_k \circ \phi_{k-1} \circ \cdots \circ \phi_1$ is an approximation to $\Phi_v(\frac{k}{L}, \cdot)$.
Similarly, $v_k$ from the discrete formulation approximates $v(\frac{k}{L}, \cdot)$.

\begin{problem}
	\label{prob:resnet-limit}
	\begin{cases}
		\text{Minimize~}& \frac{\nu}{2} \int_{0}^{1} \norm{v(t)}_\cV^2 \mathrm{d}t
		+ l(\Phi_v(1, X), Y)\\
		\text{such that~}& v \in C([0, 1], \cV)\\
	\end{cases}
\end{problem}
Here $\Phi_v(1, X)$ denotes the vector $(\Phi_v(t, X_i))_{i \in [N]}$.

\begin{theorem}
	\label{theo:v-q-continuous-problem-equivalence}
	$v$ minimizes \cref{prob:resnet-limit} if and only if $v$ fulfills
	\begin{equation}
			\dot{\Phi}^v(x, t) = \mathbf{\Gamma}(\Phi^v(x, t), q(t)) \bGamma(q(t), q(t))^{-1} \dot{q}(t)
	\end{equation}
	such that $\Phi_v(x, 0) = x$ for all $x \in X$ and $q$ minimizes \cref{prob:cont-least-action}.
\end{theorem}
The proof of this theorem is very similar to that of \cref{theo:v-q-problem-equivalence}, only that the discrete elements $q_k \in \cX^N$ are replaced by one continuous function $q: [0, 1] \rightarrow \cX^N$.
Within the scope of this thesis, we can only review the main ideas.
\begin{proof}
	Let $v$ be a minimizer of \cref{prob:resnet-limit}.
	First, define the function $q: [0, 1] \rightarrow \cX^N$ by as the vector with elements
	\begin{equation}
		q_i(t) \coloneqq \Phi_v(t, X_i) \ .
	\end{equation}
	Then $\Phi_v(1, X_i) = q_i(t)$ and thus $l(\Phi_v(1, X)) = q(1)$.
	Furthermore $\dot{q_i}(t) = \dot{\Phi}_v(t, X_i)$, or in vector notation: $\dot{q}(t) = \dot{\Phi}_v(t, X)$.
	Because $\Phi_v$ satisfies the differential equation in \cref{eq:phi-v-differential-equation}, it follows that
	\begin{equation}
		\dot{q}(t) = v(t, \Phi_v(t, X)) \ .
	\end{equation}
	Now, the idea is that if $t \rightarrow \min \norm{v(t, \Phi_v(t, \cdot))}_\cV^2$ is in $C([0, 1], \cV)$ -- which requires continuity and the global Lipschitz property -- this function is a minimizer of \cref{prob:resnet-limit}.
	For fixed $t \in [0, 1]$, we face an optimal recovery problem:
	\begin{equation}
		\min\{\norm{v(t, \Phi_v(t, \cdot))}_\cV^2\ |\ v(t, \Phi_v(t, X)) = \dot{q}(t)\} \ .
	\end{equation}
	\cref{eq:optimal-recovery-f} provides an explicit solution to this problem, which is:
	\begin{equation}
		v(t, \Phi_v(t, x)) = \bGamma(\Phi^v(t, x), q(t))\bGamma(q(t), q(t))^{-1}\dot{q}(t) \ .
	\end{equation}
	One can verify that this function is $C([0, 1], \cV)$.
\end{proof}

\subsection{Discrete Geodesic Shooting}

For completeness, we will briefly show the discrete formulation of \cref{prob:geodesic-shooting}.
The derivation involves the theory of discrete Lagrangian and Hamiltonian mechanics (see e.g. \cite{west04}) and is similar to the continuous case.
Consider the discrete Hamiltonian system
\begin{equation}
\label{eq:discrete-hamiltonian-system}
	\begin{split}
		q_{k+1} &= q_k + \Delta t \bGamma(q_k, q_k) p_k\\
		p_{k+1} &= p_k + \frac{\Delta t}{2} \grad_{q_{k+1}} \left(p_{k+1}^\T \bGamma(q_{k+1}, q_{k+1}) p_{k+1}\right)\ .
	\end{split}
\end{equation}
The discrete geodesic shooting formulation is as follows:
\begin{problem}
	\label{prob:discrete-geodesic-shooting}
	\begin{cases}
		\text{Minimize~} & \frac{\nu}{2} \sum_{k=1}^L p_k^\T \bGamma(q_k, q_k) p_k \Delta t + l(q_{L+1}, Y)\\
		\text{such that~} & p_k = \bGamma(q_k, q_k)^{-1} \frac{q_{k+1} - q_k}{\Delta t},\ q_1 = X, \Delta t = \frac{1}{L} \\
		&\text{~and~} (q_k, p_k) \text{~follow the discrete Hamiltonian equations \ref{eq:discrete-hamiltonian-system}}\ .
	\end{cases}
\end{problem}
Let $\fV_L(p_1, X, Y)$ denote the problem's objective function.
We end up with the following result.
\begin{theorem}
	\label{theo:discrete-shooting-min-q-equivalence}
	$q_1, \dots, q_{L+1}$ minimizes \cref{prob:min-q} if and only if with $p_k \coloneqq \bGamma(q_k, q_k)^{-1} \frac{q_{k+1} - q_k}{\Delta t}$, $p_1$ minimizes \cref{prob:discrete-geodesic-shooting}.
\end{theorem}

\subsection{Existence of Minimizers}

We already saw the equivalence of \cref{prob:cont-least-action,prob:geodesic-shooting,prob:resnet-limit}, in the sense that their minimizers bijectively correspond to each other -- that is, if they exist.
Now we will show that there do indeed exist such minimizers and also that the problems' minimal values are identical.
\begin{theorem}[Existence of minimizers for the continuous problems]
	\label{theo:continuous-solutions-existence}
		There exist minimal points for \cref{prob:cont-least-action,prob:geodesic-shooting,prob:resnet-limit} and their minimal values are identical.
\end{theorem}

\begin{proof}
	First, we show the existence of minimizers:
	\cref{theo:geodesic-shooting,theo:v-q-continuous-problem-equivalence} state that we can obtain a minimizer for each problem from a minimizer $p(0)$ of the geodesic shooting formulation in \cref{prob:geodesic-shooting}.
	Hence we just have to show the existence of such a $p(0)$.
	Define the ball $B_\rho \coloneqq \{p(0) \in \cX^N\ > \ \norm{p(0)}_{\cX^N}^2 \leq \rho^2 \}$ ( $\norm{\cdot}_{\cX^N}$ is the norm induced by the inner product on the product space $\cX^N$).
	Since we required $\cX$ to be finite dimensional, so is $\cX^N$.
	This implies that $B_\rho$ is compact.
	We now want to show the existence of a local minimum of $p(0) \rightarrow \fV(p(0), X, Y)$ on $B_\rho$.
	If we can show that $\fV$ is continuous in $p(0)$, this follows from the extreme value theorem.
	
	Furthermore, \cref{cond:feature-condition} (2) states that there exists an $r > 0$ such that for all $Z \in \cX^N$ $Z^\T \bGamma(X, X) Z \geq r Z^\T Z = r \norm{Z}_{\cX^N}^2$.
	\cref{cond:feature-condition} (4) requires the loss $l$ to be positive.
	These two facts imply that
	\begin{align}
		\lim_{\norm{p(0)} \rightarrow \infty} \fV(p(0), X, Y) 
		&= \lim_{\norm{p(0)} \rightarrow \infty} p(0)^\T \bGamma(X, X) p(0) + l(q(1), Y)\\
		& \geq 	\lim_{\norm{p(0)} \rightarrow \infty} p(0)^\T \bGamma(X, X) p(0) \\
		& \geq \lim_{\norm{p(0)} \rightarrow \infty} r \norm{p(0)}_{\cX^N}^2
		= \infty \ .
	\end{align}
	This means that there exists a $\rho > 0$ such that $B_\rho$ contains a global minimizer of $\fV$ and even a $\rho > 0$ such that all global minimizers are contained in a $B_\rho$.
	
	Now we are left with the proof that $\fV$ is continuous in $p(0)$.
	The first term of $\fV$ is a quadratic form in $p(0)$ and hence continuous.
	The second term -- the loss $l$ -- is continuous in $q(1)$.
	In order to see that $q(1)$ is continuous in $p(0)$ we use a result from the theory of ODEs.
	Again, write
	\begin{equation}
		\left(\dot{q}(t), \dot{p}(t)\right)^\T = f\left(t, \left(q(t), p(t)\right)^\T \right)
	\end{equation}
	for the Hamiltonian system in \cref{eq:hamiltonian-system}.
	We already know that $f$ is bounded for $t \in [0, 1]$, because $q(t)$ and $p(t)$ are.
	$f$ is continuous in $\left(q(t), p(t)\right)^\T$ and so is its Jacobian, as one can easily verify.
	The continuity of the Jacobian implies its boundedness.
	Let $(q_i, p_i)^\T$ be the unique solution to the Hamiltonian system $f$ with initial conditions $q_i(0) = X, p(0) = p_{0, i}$ for $i \in [2]$ (which exists according to \cref{theo:hamiltonian-system-solution}).
	Now, we use a result from the theory of ODEs:
	From \cite[Theorem~1.4.1]{arino06} it follows as a special case that:
	For all $\epsilon > 0$ there exists $\delta > 0$ such that:
	\begin{equation}
		\norm{p_{0, 1} - p_{0, 2}}_{\cX^{2N}} < \delta \Rightarrow 
		\left(\forall t \in [0, 1]:\ \norm{(q_1(t), p_1(t))^\T - (q_2(t), p_2(t))^\T}_{\cX^{2N}} < \epsilon \right) \ .
	\end{equation}
	Since $\norm{\cdot}_{\cX^{2N}}$ is the norm that is induced on the product space, it follows that especially $\norm{q_1(t) - q_2(t)}_{\cX^N} < \epsilon$, which proves the continuity of $q(1)$ in $p(0)$.
	
	The identity of the minimal values can be deduced directly from the theorems \cref{theo:v-q-continuous-problem-equivalence,theo:geodesic-shooting} and their proofs.
\end{proof}

An equivalent result as \cref{theo:continuous-solutions-existence} can be derived for the discrete case.
\begin{theorem}[Existence of solutions to the discrete problems]
	\label{theo:discrete-solutions-existence}
	There exist minimal points for \cref{prob:discrete-geodesic-shooting,prob:min-v-f,prob:min-q} and their minimal values are identical.
\end{theorem}
The proof is practically identical to the proof for the continuous case and will not be conducted here.

\subsection{Convergence}

It states convergence results for all three discrete problems we have dealt with so far.
Let $M_L(X, Y)$ be the set of minimizers of \cref{prob:discrete-geodesic-shooting}.
Respectively, let $M(X, Y)$ be the set of minimizers of \cref{prob:geodesic-shooting}.
\cref{theo:continuous-solutions-existence,theo:discrete-solutions-existence} imply that both sets are non-empty.
\begin{theorem}[Convergence theorem]
	\label{theo:problem-convergence}
	The minimal value of \cref{prob:discrete-geodesic-shooting,prob:min-q,prob:min-v-f} converges towards the minimal value of \cref{prob:cont-least-action,prob:geodesic-shooting,prob:resnet-limit} as $L \rightarrow \infty$.
	Furthermore, the limit of $M_L(X, Y)$ is contained in $M(X, Y)$:
	\begin{equation}
	\label{eq:limit-adherence}
		\bigcap_{L' \in \mathbb{N}} \closure\left(\bigcup_{L' \geq L} M_L(X, Y)\right) \subseteq M(X, Y) \ .
	\end{equation}
\end{theorem}
Here, $\closure (A)$ is the closure of the set $A$.
We will sketch the proof:
\begin{proof}
	From the previous equivalence theorems in this section we know that we can parameterize the minimal values and points by the initial momentum $p_1$ and $p(0)$ for the discrete \ref{prob:discrete-geodesic-shooting} and continuous geodesic shooting problem \ref{prob:geodesic-shooting} respectively.
	This means if suffices to show the convergence of $\fV_L(\cdot, X, Y)$ towards $\fV(\cdot, X, Y)$.
	Let $p_1$ minimize $\fV_L$.
	Then $p_1, \ldots, p_L$, $q_1, \ldots, q_L$ follow the discrete Hamiltonian system \ref{eq:discrete-hamiltonian-system}.
	It can be shown that this system converges uniformly to the continuous Hamiltonian system in \cref{eq:hamiltonian-system} in the sense that as $L \rightarrow \infty$, the interpolation of the solutions $p_k, q_k, k \in [L]$ form a trajectory that follows the continuous Hamiltonian system (we will not prove this here).
	This implies
	\begin{equation}
		\frac{\nu}{2} \sum_{k=1}^L p_k^\T \bGamma(q_k, q_k) p_k \Delta t + l(q_{L+1}, Y) 
		\stackrel{L \rightarrow \infty}{\longrightarrow} \frac{\nu}{2} \int_{0}^{1} p(t)^\T \bGamma(q(t), q(t)) p(t) \mathrm{d} t + l(q(1), Y) \ . 
	\end{equation}
	But because the Hamiltonian is constant along the $q, p$, the latter term is equal to $\frac{\nu}{2} p(0)^\T \bGamma(X, X) p(0) + l(q(1), Y)$.
	This shows the convergence of $\fV_L(\cdot, X, Y)$ towards $\fV(\cdot, X, Y)$, which is even uniform.
	In the proof of \cref{theo:continuous-solutions-existence} we saw that there exists a $\rho$ such that $B_\rho$ contains all global minima of $\fV$.
	This can also be shown for $\fV_L$, where $\rho$ can be chosen independently of $L$.
	Hence, choose $\rho$ such that all global minima of $\fV_L$ and $\fV$ are contained in $B_\rho$.
	Then, it follows that
	\begin{equation}
		\lim_{L \rightarrow \infty} \min_{p_1 \in B_\rho} \fV_L(p_1, X, Y) = \min_{p_1 \in B_\rho} \lim_{L \rightarrow \infty}  \fV_L(p_1, X, Y) = \min_{p_1 \in B_\rho} \fV(p_1, X, Y) \ .
	\end{equation}

	For the second part, let $(p_k)_{k \in \mathbb{N}}, p_k \in M_k(X, Y)$ be a sequence of optimal initial momenta.
	Let $p(0)$ be one of its adherence values, that is, a subsequence $(p_{k_n})_{n \in \mathbb{N}}$ converges towards $p(0)$.
	Then, $\lim_{n\rightarrow \infty} \fV_{k_n}(p_{k_n}) = \fV(p(0))$ and $p(0)$ minimizes $\fV$.
	In order to see that, let $\epsilon > 0$. 
	Because of the uniform convergence of $\fV_k$ towards $\fV$, there exists an $n_0 \in \mathbb{N}$ such that for all $n \geq n_0$:
	\begin{align}
		\abs{\fV_{k_n}(p_{k_n}) - \fV(p(0))} 
		&\leq \abs{\fV_{k_n}(p_{k_n}) - \fV_{k_n}(p(0))} + \abs{\fV_{k_n}(p(0)) - \fV(p(0))}\\
		&< \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon \ .
	\end{align}
\end{proof}

In \cref{eq:limit-adherence}, \citet{owhadi20} claims that equality holds.
However, this could not be shown here.

This concludes the section on mechanical regression.